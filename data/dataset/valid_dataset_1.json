[
    {
        "prompt": "let a herd of sheep block the taxi",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/50455b5ab8ef46578eec963b3fbe59f9_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/50455b5ab8ef46578eec963b3fbe59f9_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f5b7def631bd4e7c8736368688baced7_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9016961262134842,
        "model2_overall_score": 0.08793550441813658
    },
    {
        "prompt": "Put popcorn in the plate.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/1eab6e6f168f44a8bd2d166a40d72632_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/041e6c0073f4474fadb8d69e2ae0bba2_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/1eab6e6f168f44a8bd2d166a40d72632_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.29556744009866087,
        "model2_overall_score": 0.64284362778929
    },
    {
        "prompt": "Have the woman be wearing a blue tank top",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/b0da9e85da95400c8c2f02f35209a030_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/b0da9e85da95400c8c2f02f35209a030_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/df0532cf54194663af0b616b58c4eb69_out.jpg",
        "model1": "MagicBrush",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.49372672699083053,
        "model2_overall_score": 0.6796528554984681
    },
    {
        "prompt": "make the cat lick its nose",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/16acf19da9b544b68b2fc82423e8ca77_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/16acf19da9b544b68b2fc82423e8ca77_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/07ed8daeaf50454bbfc57aeb314fa555_out.jpg",
        "model1": "PNP",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.984945963388787,
        "model2_overall_score": 0.9916179593040713
    },
    {
        "prompt": "change strawberries to oranges",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/063a9c68ab6b4854bd2f3d4c8bba76ff_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/063a9c68ab6b4854bd2f3d4c8bba76ff_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/280f5255611d46fb926a8fa090aba818_out.jpg",
        "model1": "CosXLEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.33236270894522524,
        "model2_overall_score": 0.7552045814915542
    },
    {
        "prompt": "Make it a black sheep.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/d20b9272bc1c40a8ac6848650f68ed7a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d20b9272bc1c40a8ac6848650f68ed7a_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/38f7a973a22641b6837b9f1ab7a1af36_out.jpg",
        "model1": "MagicBrush",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7850507780468867,
        "model2_overall_score": 0.19862927578017708
    },
    {
        "prompt": "change strawberries to oranges",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/cb5f45e85f1b43939022669037a1158d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/cb5f45e85f1b43939022669037a1158d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/b446f8adb5d949f6bfa30df26c16cba9_out.jpg",
        "model1": "InfEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.46050593131588957,
        "model2_overall_score": 0.015674677133397874
    },
    {
        "prompt": "Put bride and groom toppers on it.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/014a9d69efb34077aad237f6755f8620_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/014a9d69efb34077aad237f6755f8620_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/0772f2fa634341b7845b353fda64fdb4_out.jpg",
        "model1": "SDEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9536195879344039,
        "model2_overall_score": 0.6076196993504562
    },
    {
        "prompt": "Make the man's top blue.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f7fa29fd1eb8491289736d7478f1b513_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/c91c89f8e5b14d20aecda503d022f600_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f7fa29fd1eb8491289736d7478f1b513_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.84685858831762,
        "model2_overall_score": 0.1241032478145625
    },
    {
        "prompt": "Make the donut a cupcake.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f85569309d904ccbbcba6d9b5bef5c29_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7fa64958edf04993a59830be36535754_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f85569309d904ccbbcba6d9b5bef5c29_out.jpg",
        "model1": "InfEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.16875384518411396,
        "model2_overall_score": 0.17807077335500987
    },
    {
        "prompt": "Have the cow wear a hat.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/af0ea7d0bec04fae8dc393a6aab4cb09_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/af0ea7d0bec04fae8dc393a6aab4cb09_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ccfd2508103442ba8f039fcae5e24258_out.jpg",
        "model1": "CosXLEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.03434833334306575,
        "model2_overall_score": 0.48338267322219963
    },
    {
        "prompt": "give her a baseball cap",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ddd2bbf0e54541febcf5f091d5eeb08b_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ddd2bbf0e54541febcf5f091d5eeb08b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/2ccebe1daad74eb090cca0e322fbcb23_out.jpg",
        "model1": "MagicBrush",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.28063652554245744,
        "model2_overall_score": 0.1689688238725192
    },
    {
        "prompt": "let the cat have blue eyes",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/229cbba1b9d34dbeb95a83ee98ecd28f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/dbc33908043c4dba9abf1af31c8f9f2c_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/229cbba1b9d34dbeb95a83ee98ecd28f_out.jpg",
        "model1": "PNP",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7969246276869942,
        "model2_overall_score": 0.9185095444324037
    },
    {
        "prompt": "let the kid sleep",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/0f7500aff81f4aa185bf715c23e5e12f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/dfa395f396814a93b91531366c011e4e_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/0f7500aff81f4aa185bf715c23e5e12f_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.2539492855308171,
        "model2_overall_score": 0.3915240270261203
    },
    {
        "prompt": "make the ground a forest instead of a slope.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/5fe64cc5712a4088ab45c20868c4d6bb_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/5fe64cc5712a4088ab45c20868c4d6bb_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/e6e43e504f2a4ba681a174f9214e86ff_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8153356989261219,
        "model2_overall_score": 0.46039930227942283
    },
    {
        "prompt": "Replace the coffee with beer.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/b7b10c9369ea46edbd5ca77d86a0cda9_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/b7b10c9369ea46edbd5ca77d86a0cda9_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/2eee610cc9074ee9b8f155343f2cae32_out.jpg",
        "model1": "PNP",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8532875088452672,
        "model2_overall_score": 0.8181693927787538
    },
    {
        "prompt": "give her a skirt to wear",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/b4fe81a30b104fbeb0f7ea802cf4e18a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/a65d40e83a044d989cda6afc2ee32766_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/b4fe81a30b104fbeb0f7ea802cf4e18a_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7559197506379324,
        "model2_overall_score": 0.04426648836942426
    },
    {
        "prompt": "Make the zebra a regular horse.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/d23638e69ac247c1872204e1b9e78f92_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d23638e69ac247c1872204e1b9e78f92_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ab9020af2e4c4987aef2befee1db4b31_out.jpg",
        "model1": "SDEdit",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8559264732691435,
        "model2_overall_score": 0.18552461166569834
    },
    {
        "prompt": "There should be some cutlery on the table.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/7deb9717144d462481ecae933a0221e3_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/0deff1f0356f43558de96bbdc06677b9_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/7deb9717144d462481ecae933a0221e3_out.jpg",
        "model1": "SDEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3611604239357098,
        "model2_overall_score": 0.7107767297213314
    },
    {
        "prompt": "add water and flowers in the tub",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/3c1c9d5554704c66a6cfc09da98d8445_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/c4b988b9594d4312a331f7c66c66186a_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/3c1c9d5554704c66a6cfc09da98d8445_out.jpg",
        "model1": "MagicBrush",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.14991253934993765,
        "model2_overall_score": 0.2717248886128707
    },
    {
        "prompt": "Replace the greens with onions.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/52385d6254cb4ab2b30e1375e10081bc_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/52385d6254cb4ab2b30e1375e10081bc_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/717953aaa83e4af7b28b8aed856cbbc9_out.jpg",
        "model1": "MagicBrush",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8130308732759678,
        "model2_overall_score": 0.4313681750175704
    },
    {
        "prompt": "let a child play nearby",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/5eff0d87c1a2442cb1d835aff85447c1_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/5eff0d87c1a2442cb1d835aff85447c1_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/0760e66fe46e4803b9928f4b901a7922_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8647107804129196,
        "model2_overall_score": 0.30277427238359345
    },
    {
        "prompt": "Make it a slice of pizza instead of the sandwich.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/e7b33c7e71c74909b0d4b1cc1c5f8e8e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/2d594bac285a4a3e9b19e535c4ebcabf_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/e7b33c7e71c74909b0d4b1cc1c5f8e8e_out.jpg",
        "model1": "PNP",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5049213546008118,
        "model2_overall_score": 0.7120739561754167
    },
    {
        "prompt": "let the woman have blonde hair",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/b64c7c5d4c6a4d5eabdd0183e0f8288a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/3f41ddccf8ae42a6a31dda716fc52838_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/b64c7c5d4c6a4d5eabdd0183e0f8288a_out.jpg",
        "model1": "InfEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7102476724819711,
        "model2_overall_score": 0.4935432660910053
    },
    {
        "prompt": "make it a pepperoni pizza",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/0ff21c16e50d4e60be9d6ffd76c59b2a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/14bee6c159114f45aa314dad73d5eac3_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/0ff21c16e50d4e60be9d6ffd76c59b2a_out.jpg",
        "model1": "PNP",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7023492804742797,
        "model2_overall_score": 0.332839511954933
    },
    {
        "prompt": "change the green ball to blue",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/23cb87a618bf4b08bd4a93324e67d04c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/58c72e18c3f24958a2df5a3d6e5fbf3b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/23cb87a618bf4b08bd4a93324e67d04c_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8842067207455169,
        "model2_overall_score": 0.17236585135940563
    },
    {
        "prompt": "Have the woman be wearing a blue tank top",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/aa6b5351ea7146768f1757f0d91e6db0_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/c2a1d87a0ad54f04bd47a25db8cda33e_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/aa6b5351ea7146768f1757f0d91e6db0_out.jpg",
        "model1": "InfEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.30722677111490926,
        "model2_overall_score": 0.16324471655023476
    },
    {
        "prompt": "Add a cup of coffee.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/d42615e848a144cd9d6997c113e637b2_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d42615e848a144cd9d6997c113e637b2_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/16da994be8724f04b46c1075d76a22cb_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.2836014860078476,
        "model2_overall_score": 0.367847692814695
    },
    {
        "prompt": "Make the dog's eyes closed.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/7042241f2b8b43e2abd750d963e7d3ae_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7042241f2b8b43e2abd750d963e7d3ae_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ca2e508bb86844858e5cc9fbc9210d34_out.jpg",
        "model1": "PNP",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9936714183023795,
        "model2_overall_score": 0.7897914041636233
    },
    {
        "prompt": "Let the monitor turn black.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/c216570dfbf04784bb50b35bfcb4b67a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/fab1c33e41e24baa9c51b9fb7315c603_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/c216570dfbf04784bb50b35bfcb4b67a_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5190370404552348,
        "model2_overall_score": 0.23113937050503064
    },
    {
        "prompt": "add a woman inside the car",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/97df8a13f75846a587d68cc2f41330c7_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/1a83d5ce710649b5b5d1a01ae24cfa27_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/97df8a13f75846a587d68cc2f41330c7_out.jpg",
        "model1": "MagicBrush",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5130485569190092,
        "model2_overall_score": 0.764705446677895
    },
    {
        "prompt": "Make it a black sheep.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/4bf702e14d994dfda89c55583d133081_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/4bf702e14d994dfda89c55583d133081_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/92aa9a57b8db433c92c18499b5fdadf8_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5062499724389637,
        "model2_overall_score": 0.3023187576356072
    },
    {
        "prompt": "put the zebras next to a river",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/c11fd2ce63854226b290c8267dd0a4ba_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/c11fd2ce63854226b290c8267dd0a4ba_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/16757f26b01d4703993d32ef2b06d167_out.jpg",
        "model1": "CosXLEdit",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.11972393255168967,
        "model2_overall_score": 0.9132432441731485
    },
    {
        "prompt": "change the teddy bear into a stuffed action figure toy",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/a2e04971734b4243be670ab5c954508c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/a2e04971734b4243be670ab5c954508c_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/e1bab6f1a52b4490a801a8607d32197f_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.40383553673616535,
        "model2_overall_score": 0.49857128877500534
    },
    {
        "prompt": "let there be granite floor in the kitchen",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/922630390c9e4f3f9deddff9fe179341_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/0ead96dee61c4473b2fc9be72d1b1aed_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/922630390c9e4f3f9deddff9fe179341_out.jpg",
        "model1": "CosXLEdit",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.43725921890316777,
        "model2_overall_score": 0.3148512629937207
    },
    {
        "prompt": "edit the background by removing the museum and placing a castle",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/269f94408991400794b66d112fd31d82_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/41a69b2f181e47be83be4c9506d7bc01_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/269f94408991400794b66d112fd31d82_out.jpg",
        "model1": "SDEdit",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8931334179128775,
        "model2_overall_score": 0.5536333400377496
    },
    {
        "prompt": "let the woman have blonde hair",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/66233a30931c4277b05fd748a50c7356_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/377de835ff174c8c9f31fc82be7a7fb2_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/66233a30931c4277b05fd748a50c7356_out.jpg",
        "model1": "CosXLEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6850516644992928,
        "model2_overall_score": 0.39209731454542107
    },
    {
        "prompt": "A dog should be near the sheep.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f0c7d6be0fd041f18496e6b9b13be4b2_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f0c7d6be0fd041f18496e6b9b13be4b2_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/e81257e5eb8e4564a8fa7027c3cd5dba_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.617164883742867,
        "model2_overall_score": 0.6238913755441322
    },
    {
        "prompt": "Have the cow wear a hat.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/833bebf7c2094f0b80626858fa66c963_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/af61fccaa8f74c8caa2c21d616da965c_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/833bebf7c2094f0b80626858fa66c963_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6568401024489201,
        "model2_overall_score": 0.9601861963968381
    },
    {
        "prompt": "It could be a microwave next to the woman.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/919ff92d79114f4ab0a659b0927664d0_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d58cfb695f9c4b84b452bf7fb852bfc9_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/919ff92d79114f4ab0a659b0927664d0_out.jpg",
        "model1": "InfEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.17400173362778815,
        "model2_overall_score": 0.30750940161232587
    },
    {
        "prompt": "Change the trees to palm trees",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/a126053b1be740c28c9dcdab25b90f6c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/a126053b1be740c28c9dcdab25b90f6c_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/c9049809ad3942b59b83838fea24869e_out.jpg",
        "model1": "PNP",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9642644020443939,
        "model2_overall_score": 0.7217580042988581
    },
    {
        "prompt": "add water and flowers in the tub",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ad60067df8c942b99ba77f5f5a41b342_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/6b4142db4e8e4cb3a3765ccd47f7420c_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ad60067df8c942b99ba77f5f5a41b342_out.jpg",
        "model1": "SDEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5679186492623718,
        "model2_overall_score": 0.7221419073321174
    },
    {
        "prompt": "let the cup contain flowers",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/513fa156073d4467914ec7e3ed6f5a85_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/513fa156073d4467914ec7e3ed6f5a85_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/575383fd61684dc382e9819c5ea26b25_out.jpg",
        "model1": "InfEdit",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9257600494983695,
        "model2_overall_score": 0.8267543331131392
    },
    {
        "prompt": "Put a horse insted of the goats.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/47b0af52909241dda579ec07000a32a8_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/50398e107c9b4f34b1d1a7304b22e485_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/47b0af52909241dda579ec07000a32a8_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6924296988248868,
        "model2_overall_score": 0.28412741860303325
    },
    {
        "prompt": "make the woman hold a camera",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/81f2e411d7cc4415a8da2092f4c63229_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/85cd7aa4c88b423f976a835b99ef8db9_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/81f2e411d7cc4415a8da2092f4c63229_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6544731128266731,
        "model2_overall_score": 0.6232854446138191
    },
    {
        "prompt": "give the person a bowl",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/5861fa45d8f54749ad96d15ebe4e28da_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/5861fa45d8f54749ad96d15ebe4e28da_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ddf5189279454c6996c860f25448f869_out.jpg",
        "model1": "SDEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7615794812104525,
        "model2_overall_score": 0.40774820090563624
    },
    {
        "prompt": "change the bag of chips into a backpack",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/731cb91d4b9e480ba35ce14937e9d81c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/64a3a52897aa4d3f94b91fdc27677dce_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/731cb91d4b9e480ba35ce14937e9d81c_out.jpg",
        "model1": "MagicBrush",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.19163886761991533,
        "model2_overall_score": 0.9396805017287281
    },
    {
        "prompt": "Have there be a dolphin jumping out of the water",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/4d605bc58aae4a739a56b886292cd81d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/4d605bc58aae4a739a56b886292cd81d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/3b23168e12974ca7b2243f88a2530513_out.jpg",
        "model1": "PNP",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.699025519252827,
        "model2_overall_score": 0.08417043833963522
    },
    {
        "prompt": "change the stuffed toys into rubber duckies",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/afc3a2ea037645bda9de66f4c62ecf94_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/234dc353d0e74308be2292d567135db1_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/afc3a2ea037645bda9de66f4c62ecf94_out.jpg",
        "model1": "InfEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.38567088947704464,
        "model2_overall_score": 0.1844177789475162
    },
    {
        "prompt": "Put a pond next to the elephant",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/51a4cdcdc99942d4a26665b06191c0a4_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/51a4cdcdc99942d4a26665b06191c0a4_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/c2fa49ad896244608860185cd6abdd41_out.jpg",
        "model1": "MagicBrush",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.29531913126702447,
        "model2_overall_score": 0.4564732712502336
    },
    {
        "prompt": "Make the cake a chocolate cake",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/9017774c8b5d4236b28ff1e31b4e5421_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/bb0844fd94674792b175cf320255e794_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/9017774c8b5d4236b28ff1e31b4e5421_out.jpg",
        "model1": "PNP",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5537859655741055,
        "model2_overall_score": 0.6261306015847963
    },
    {
        "prompt": "edit some mountains in the background",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/de8a4a650ba84b98afa98a405d890ece_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7640e618f58e41a79d69c80c1a333d4b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/de8a4a650ba84b98afa98a405d890ece_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.4598187266513898,
        "model2_overall_score": 0.4114865775021508
    },
    {
        "prompt": "add a pedestrian",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/79bc695e451a4401ba2a662a9fb2ce1e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/46547552fcc84b0689ac470a49c3274f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/79bc695e451a4401ba2a662a9fb2ce1e_out.jpg",
        "model1": "InfEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9007264000951025,
        "model2_overall_score": 0.24612996964173905
    },
    {
        "prompt": "Let the bluebery cake be topped with chocolate syrup.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/55a149fdb1e741d199806a7d99bc26f2_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/55a149fdb1e741d199806a7d99bc26f2_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/75a3cf13f1f742aea64017a7389a5a0b_out.jpg",
        "model1": "PNP",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7059888585798021,
        "model2_overall_score": 0.5891552386468301
    },
    {
        "prompt": "Let the van turn black.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/45ea10be1f804d7eb04f602e8c71a6c7_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/45ea10be1f804d7eb04f602e8c71a6c7_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/51ca514eb8974a05bee671fb2fe5a09c_out.jpg",
        "model1": "MagicBrush",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.269014289922504,
        "model2_overall_score": 0.8402533777789015
    },
    {
        "prompt": "Put a policeman in the intersection.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/d37e5796ac0f4b5e9e7016f06ee301bd_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/6359a7c6427f46da808081908bdfab18_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/d37e5796ac0f4b5e9e7016f06ee301bd_out.jpg",
        "model1": "CosXLEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.09854007325114233,
        "model2_overall_score": 0.30582740052227164
    },
    {
        "prompt": "Make the zebra a regular horse.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/bfbd7ab6c9d74b42b068acc76912b418_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/bfbd7ab6c9d74b42b068acc76912b418_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6a268891d6a14d8cb315a9b21bb7bea4_out.jpg",
        "model1": "InfEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6888510083732664,
        "model2_overall_score": 0.31603201601860387
    },
    {
        "prompt": "make the ramp cement",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/46a508f75fdc4567bbfd82da1ca3ac97_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/46a508f75fdc4567bbfd82da1ca3ac97_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f34d5ebc48df4989bf98c12b205672fb_out.jpg",
        "model1": "CosXLEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.652483159272651,
        "model2_overall_score": 0.11543671756609131
    },
    {
        "prompt": "let the cat wear a bow tie instead of a tie",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ba0c62dd61bc4bdb806b66dbb0b93444_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/669f9973859f47e69ee2ea2d5411c830_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ba0c62dd61bc4bdb806b66dbb0b93444_out.jpg",
        "model1": "InfEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9695111250653864,
        "model2_overall_score": 0.7211028590283665
    },
    {
        "prompt": "let a woman in a bridal gown stand near the cake",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/5da6e72a26524eca88d614e829809a28_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/898534f66e1c4510b77765f3a7a7b47b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/5da6e72a26524eca88d614e829809a28_out.jpg",
        "model1": "MagicBrush",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.16151895840007802,
        "model2_overall_score": 0.9507230552103182
    },
    {
        "prompt": "it should be a tennis ball on the glove.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f52029a430d34c809a764295ad30bdc8_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/a3af1e067d19447a9d9f3b736232f019_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f52029a430d34c809a764295ad30bdc8_out.jpg",
        "model1": "CosXLEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.057836628134050105,
        "model2_overall_score": 0.8333597730503284
    },
    {
        "prompt": "Make the man's top blue.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/356ee5c903df4aa4b3fdf67e80436ce2_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/356ee5c903df4aa4b3fdf67e80436ce2_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/5487f4db6f63412a814631b2970a6d4f_out.jpg",
        "model1": "MagicBrush",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.2422191768610118,
        "model2_overall_score": 0.6051983038107045
    },
    {
        "prompt": "change the toothpicks into candles",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/18637f2682ed4106a8b42f6971dd07a2_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/0531c0a51bb24bdf9189f98b7b31dc1d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/18637f2682ed4106a8b42f6971dd07a2_out.jpg",
        "model1": "InfEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.4148926215656865,
        "model2_overall_score": 0.45305218350180365
    },
    {
        "prompt": "Replace the greens with onions.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/981776d89a384bf792c8f70a6522e700_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7502e27cd29f40d999e8349befecdef0_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/981776d89a384bf792c8f70a6522e700_out.jpg",
        "model1": "CosXLEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9261843739221735,
        "model2_overall_score": 0.20763296637846584
    },
    {
        "prompt": "edit some mountains in the background",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/28baa12b60b94c5dbdd829f53883cf02_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/69de287392ab45cc920118d3f27fc12f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/28baa12b60b94c5dbdd829f53883cf02_out.jpg",
        "model1": "CosXLEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.27200383874497447,
        "model2_overall_score": 0.5047910384403895
    },
    {
        "prompt": "add a plane in the sky",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/6b62460aad06433bb2feb1968b0abecd_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/6b62460aad06433bb2feb1968b0abecd_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ba3735b5e9544f3ea8db48e5ba31c2e7_out.jpg",
        "model1": "InfEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9484202476306943,
        "model2_overall_score": 0.8085071652276172
    },
    {
        "prompt": "replace the donuts with fruits",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/1b57cf8aa3a84a26bad001b1695daf2a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/1b57cf8aa3a84a26bad001b1695daf2a_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/c72e801a79b940c881bf737a20119544_out.jpg",
        "model1": "InfEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8906268002835461,
        "model2_overall_score": 0.2829174542717192
    },
    {
        "prompt": "What if the man had a hat?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/c55c3400c6b245bca3ae1e39efcd6389_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/eea5177c671740bda628beed4f07193f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/c55c3400c6b245bca3ae1e39efcd6389_out.jpg",
        "model1": "CosXLEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.37956044311266124,
        "model2_overall_score": 0.21173160532055935
    },
    {
        "prompt": "What if the man had a hat?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ef9c8aac3f7543b19f69065f5a9fbb5e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ef9c8aac3f7543b19f69065f5a9fbb5e_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6281f453d2a04f46b0db2a491c250af7_out.jpg",
        "model1": "MagicBrush",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8635180474525104,
        "model2_overall_score": 0.5803019230819394
    },
    {
        "prompt": "Have the person swing the racquet between his legs",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/e6ae70ac35864cd183f29f9f30d12510_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/38f5e6252d6e4ac0a3e8593c14d41768_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/e6ae70ac35864cd183f29f9f30d12510_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.43233970375279573,
        "model2_overall_score": 0.26385897183050777
    },
    {
        "prompt": "Let the monitor turn black.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/d81ca44055094362bf50c318ed4c150f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d81ca44055094362bf50c318ed4c150f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/b486f8ac48034d5f90d0cc0415fafe56_out.jpg",
        "model1": "MagicBrush",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7675963600369313,
        "model2_overall_score": 0.717335242873219
    },
    {
        "prompt": "put a robot tiger next to the bear",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/c56e5c4708994b359784c8a439a9d118_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/c56e5c4708994b359784c8a439a9d118_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6e568ba608604e03b21b6113b9b92e40_out.jpg",
        "model1": "SDEdit",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3280562848574178,
        "model2_overall_score": 0.15009112519997003
    },
    {
        "prompt": "let a herd of sheep block the taxi",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/5705b782829e4649b1f468ac37f04658_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/55cb0ff6016944b0b1e2309319de49ad_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/5705b782829e4649b1f468ac37f04658_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6868008132606965,
        "model2_overall_score": 0.9115703809867005
    },
    {
        "prompt": "make it a pepperoni pizza",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/77250fa5e5c54f0c85f3b70c4223f9b4_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/9140e55300c748588bb59805f0d8d7f3_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/77250fa5e5c54f0c85f3b70c4223f9b4_out.jpg",
        "model1": "InfEdit",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9468765608128352,
        "model2_overall_score": 0.07027008452095063
    },
    {
        "prompt": "Let the bluebery cake be topped with chocolate syrup.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/98f1405c3d244208aed79aef546610c2_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/98f1405c3d244208aed79aef546610c2_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/0d6e7734ef8141ab9fabe541b1719952_out.jpg",
        "model1": "PNP",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.13670118357440153,
        "model2_overall_score": 0.080652646163903
    },
    {
        "prompt": "let the plate contain ice cream",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/8d3d7167b12448a382c91361e0961ab4_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/8d3d7167b12448a382c91361e0961ab4_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/2736bc27f4c34725aa74e6474cbd2c50_out.jpg",
        "model1": "InfEdit",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6808523006614989,
        "model2_overall_score": 0.27276119987744074
    },
    {
        "prompt": "Put a pond next to the cows.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/e7dd9898e2af446ead7f5f09b1fd56c6_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ca4c9684bbe84de799e84a508ef48e05_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/e7dd9898e2af446ead7f5f09b1fd56c6_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8979261666691842,
        "model2_overall_score": 0.21018481420964008
    },
    {
        "prompt": "let the plate contain ice cream",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/8eceafd6dc714736bbf67e30c78545a6_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/8eceafd6dc714736bbf67e30c78545a6_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/2ae82f7b1de14ee3b599504d8f3802cf_out.jpg",
        "model1": "PNP",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.1956174573829217,
        "model2_overall_score": 0.8794515761317162
    },
    {
        "prompt": "Replace the coffee with beer.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/0abcb6b4dbf14146ad49caf278a1b311_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/0abcb6b4dbf14146ad49caf278a1b311_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/9ed52df2d3644208b0339272f45cda61_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.12217711897645644,
        "model2_overall_score": 0.9030190659101832
    },
    {
        "prompt": "let the woman cry",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/1828577fdbec47f39565ddb22b03a898_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/1828577fdbec47f39565ddb22b03a898_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/cc3a18195e1e4837a3894ac7b38974d4_out.jpg",
        "model1": "InfEdit",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.058000717624732245,
        "model2_overall_score": 0.938288738687938
    },
    {
        "prompt": "make the plate empty",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/d48057ab61864eeaa01536196bcf5a8d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d0b9a856cd804aa8ac8ea4100aaa2106_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/d48057ab61864eeaa01536196bcf5a8d_out.jpg",
        "model1": "MagicBrush",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.09526872206588333,
        "model2_overall_score": 0.16662415743183367
    },
    {
        "prompt": "add a woman inside the car",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/64230f1676a74a7cbe9048688a4376cc_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/64230f1676a74a7cbe9048688a4376cc_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/01ae02fca57d46ebb5f1c5746a172642_out.jpg",
        "model1": "CosXLEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.512012983192378,
        "model2_overall_score": 0.3583170569690094
    },
    {
        "prompt": "add flying eagles over the statue",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/cda5c3239b5f4629936c41963efa1fd7_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f68316456e364ada949b5f6ebb5dc7ae_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/cda5c3239b5f4629936c41963efa1fd7_out.jpg",
        "model1": "CosXLEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.28209231973881077,
        "model2_overall_score": 0.44247048186084437
    },
    {
        "prompt": "change the green ball to blue",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/5024037b22ff436c913a3342c6813d27_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/5024037b22ff436c913a3342c6813d27_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/044271597ebc45eb9b0323a11dd129ed_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5176625897915745,
        "model2_overall_score": 0.6380175720177762
    },
    {
        "prompt": "Have the man be wearing a kilt",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/9f43f0e42c9545cba653d1cca76bff4c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/9f43f0e42c9545cba653d1cca76bff4c_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/b127d70b53ff4d919f372c10252f990b_out.jpg",
        "model1": "PNP",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.14611996291707752,
        "model2_overall_score": 0.6171369830088914
    },
    {
        "prompt": "turn the frisbee into a soccer ball",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/7647f1371a2a4823aae33f600f50112e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7647f1371a2a4823aae33f600f50112e_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/17be6b0186a3482c9887af397ba6ec12_out.jpg",
        "model1": "CosXLEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.4008813120776882,
        "model2_overall_score": 0.4023237964298362
    },
    {
        "prompt": "place a cat in the counter",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/05d814f932d64705a97dd11b435fabd2_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/05d814f932d64705a97dd11b435fabd2_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/253d4e2424ce4dfd97b5f401de579144_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.542603677941,
        "model2_overall_score": 0.4687023648254527
    },
    {
        "prompt": "Can we have a blue airplane?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/d800877c6e4641208f8087f03bd2e1fb_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d800877c6e4641208f8087f03bd2e1fb_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ead696614205401c8a4658c51468e91a_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8561157207881224,
        "model2_overall_score": 0.8885409980726723
    },
    {
        "prompt": "change the Washington Monument with the Statue of Liberty",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/c6c386d8b66a4d81afea03b4cc4876ff_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/95003010a98d45149994e19492a4908d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/c6c386d8b66a4d81afea03b4cc4876ff_out.jpg",
        "model1": "CosXLEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.23173467709339535,
        "model2_overall_score": 0.38716990690994657
    },
    {
        "prompt": "edit some mountains in the background",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/abf2ca15e9de46259e20dd331b0a24c4_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/abf2ca15e9de46259e20dd331b0a24c4_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/1bbef448940d4b9fb20d92a9effb0ee0_out.jpg",
        "model1": "MagicBrush",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.007072290952894189,
        "model2_overall_score": 0.39079043132134894
    },
    {
        "prompt": "Can we have a blue airplane?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/c75da41c35fb4004bec25af98be52050_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/c75da41c35fb4004bec25af98be52050_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/24a9f383486748b9ad7d57896d6e3f65_out.jpg",
        "model1": "InfEdit",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9523451226766589,
        "model2_overall_score": 0.49918609766007105
    },
    {
        "prompt": "let the bed be wooden",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ab13646bf2f843c999150fa0b5ad1f26_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ab13646bf2f843c999150fa0b5ad1f26_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/fc7223f504d04ef4a1746044d9b2e6b2_out.jpg",
        "model1": "InfEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3264800225997918,
        "model2_overall_score": 0.04672650128115308
    },
    {
        "prompt": "change the green ball to blue",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/783ad438ec8c45e28e3ecf5a5f29528e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/783ad438ec8c45e28e3ecf5a5f29528e_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/b894b0edb4854edb879c4195f3975553_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6013089766154189,
        "model2_overall_score": 0.5178444034326383
    },
    {
        "prompt": "let a child play nearby",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/45b8e582fed24ca7b75ae3acf3dcc04a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/68df56c535454c20bff0873b7daee08d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/45b8e582fed24ca7b75ae3acf3dcc04a_out.jpg",
        "model1": "InfEdit",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7549408289508073,
        "model2_overall_score": 0.7647063941392307
    },
    {
        "prompt": "Change the boy to a girl.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/85fcbc60d86345ea99c47a246c39842e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/85fcbc60d86345ea99c47a246c39842e_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/a437dc5592a2426ea396f055b0ee90ab_out.jpg",
        "model1": "PNP",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8541345125032239,
        "model2_overall_score": 0.7950563595241238
    },
    {
        "prompt": "Can we have mountains on the background?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ed1e78717fa745c2a6b816235333ec35_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ed1e78717fa745c2a6b816235333ec35_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/0d4ac66c0747473b9ff8bb108bd41173_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8841331931351359,
        "model2_overall_score": 0.6158264842346527
    },
    {
        "prompt": "add a car instead of motorcycles",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/d14db7a51d9c4922913b53a15043cf8e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d14db7a51d9c4922913b53a15043cf8e_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/219e1363ff4c4284b712525ba9305a55_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5388888163163847,
        "model2_overall_score": 0.4569172312748324
    },
    {
        "prompt": "Make the dog's eyes closed.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/6237a13637a44915978d9958c9edde2d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/5def68f63bd747e1b13e43db0c293f67_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6237a13637a44915978d9958c9edde2d_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5142622012850052,
        "model2_overall_score": 0.2888949848403829
    },
    {
        "prompt": "let the patch of flowers be daffodils",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/e25b5fec90b8445c922a21e9668616ee_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/55674f9069d2468eb7fb5d80ce8b3986_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/e25b5fec90b8445c922a21e9668616ee_out.jpg",
        "model1": "Pix2PixZero",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7732229715791514,
        "model2_overall_score": 0.6081706784940962
    },
    {
        "prompt": "add a street",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/df386da0755b42bb8102b611567ae61d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/855baeda9e70448694297a5ef621e9ed_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/df386da0755b42bb8102b611567ae61d_out.jpg",
        "model1": "MagicBrush",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3553143363715432,
        "model2_overall_score": 0.4966504602944868
    },
    {
        "prompt": "Put a policeman in the intersection.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/0ef29a2be4e74b9dacd5fad988edd09b_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/54717b374f67473d9604883c0f8d0d36_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/0ef29a2be4e74b9dacd5fad988edd09b_out.jpg",
        "model1": "InfEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.32935410460027414,
        "model2_overall_score": 0.8309966224781977
    },
    {
        "prompt": "remove the tent and add a bonfire",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/5e6c34a1d18d49b2b83724ea2da2e835_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/5e6c34a1d18d49b2b83724ea2da2e835_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/b646722f296741a6a381753f68ea24c3_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.27037195439498873,
        "model2_overall_score": 0.08743123936484576
    },
    {
        "prompt": "Have the person swing the racquet between his legs",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/7a260033ffbd468c84816361facb0fa3_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7a260033ffbd468c84816361facb0fa3_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/37506d729b59422f8538dc1d3aeb4e24_out.jpg",
        "model1": "InfEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.29611514072327205,
        "model2_overall_score": 0.8370804804302291
    },
    {
        "prompt": "Let water run from the faucet.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/78ffb93859bc4a2d93e431e25384d318_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/78ffb93859bc4a2d93e431e25384d318_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/d90afa4b8c7b4bb3a1847cc16f4d5ad6_out.jpg",
        "model1": "CosXLEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6283292378795272,
        "model2_overall_score": 0.2947579541655355
    },
    {
        "prompt": "Make all the grass green.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/d01447ac465d4f6db41efe848a07d0f1_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/22c39f4f40824962835d5a4376542993_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/d01447ac465d4f6db41efe848a07d0f1_out.jpg",
        "model1": "MagicBrush",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.524628189199596,
        "model2_overall_score": 0.8851209658252677
    },
    {
        "prompt": "What if the man had a hat?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/04d7d86c11d24440942b7e22f6f03bfe_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/acd81612e0914b7dbdd1d32d568723bb_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/04d7d86c11d24440942b7e22f6f03bfe_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.16016871676675604,
        "model2_overall_score": 0.9513574990106431
    },
    {
        "prompt": "Let the monitor turn black.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/2a99f73446d3406f9fcfbd21ee618e06_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/2a99f73446d3406f9fcfbd21ee618e06_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/92b5a288e1534b58be6f30937ec1320e_out.jpg",
        "model1": "MagicBrush",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6369079636043223,
        "model2_overall_score": 0.7775705305549155
    },
    {
        "prompt": "change the trees at the back into palm trees",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/de692a7e881049508c590b3c0975b35d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/de692a7e881049508c590b3c0975b35d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6c81ea38857444bb93f66d1a5d7896d7_out.jpg",
        "model1": "CosXLEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.002164441457535249,
        "model2_overall_score": 0.22449458652156595
    },
    {
        "prompt": "Add a giant wasp.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/4f6b0acbc1ed494696dab619d7708bf7_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/4f6b0acbc1ed494696dab619d7708bf7_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/dd540b831780463892abac46e46970e2_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.442289738102538,
        "model2_overall_score": 0.37227252083057594
    },
    {
        "prompt": "let the woman cry",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/80ee9351506c433e89a00e29a699550b_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/80ee9351506c433e89a00e29a699550b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/503c388c373f4327bfd6ba9527e04678_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5760466977101746,
        "model2_overall_score": 0.06316449789244061
    },
    {
        "prompt": "Make the apples green",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/1f0a63c38e7745b3b42343592937f51c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/5ed3f7df7efb44eea9c688b683708eb9_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/1f0a63c38e7745b3b42343592937f51c_out.jpg",
        "model1": "PNP",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9861904311131189,
        "model2_overall_score": 0.5964023572232043
    },
    {
        "prompt": "Get rid of the jar of cookies",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/8a34fe8c57d64b8d93faa0acfd478220_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/8a34fe8c57d64b8d93faa0acfd478220_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/c428477ba79a4fdabb121d487cf40b77_out.jpg",
        "model1": "PNP",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9260377338083773,
        "model2_overall_score": 0.34921819670043164
    },
    {
        "prompt": "Make the zebra a regular horse.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/651e6e035a76431793e15e4926d17823_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/651e6e035a76431793e15e4926d17823_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/a342bc1c6d9c4e01990c464ce7179ffc_out.jpg",
        "model1": "InfEdit",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.45780732451294737,
        "model2_overall_score": 0.7567985825435365
    },
    {
        "prompt": "A dog should be near the sheep.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/b44f5eaa33c84117805c6e320cb82b8e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/b44f5eaa33c84117805c6e320cb82b8e_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/7829fb2a47a34407a29ae9ab31e9e447_out.jpg",
        "model1": "MagicBrush",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.4884662477667827,
        "model2_overall_score": 0.9550761498497355
    },
    {
        "prompt": "Put a show about cats on the TV.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f035bc88f31e49fbae22459fbfa1707a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f035bc88f31e49fbae22459fbfa1707a_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/c17a575515e341ef95fb569c2c3b3567_out.jpg",
        "model1": "MagicBrush",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.22228509906541172,
        "model2_overall_score": 0.6542306265643505
    },
    {
        "prompt": "make the ramp cement",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/27d0332750cc4c3582e52f2825e2e4cd_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/fe741f7b42004c41a75347136d173aa3_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/27d0332750cc4c3582e52f2825e2e4cd_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3283664089182101,
        "model2_overall_score": 0.3059021393192485
    },
    {
        "prompt": "put strawberry on the plate",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/d2c71b3a11b84034baaf1d280146e928_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/77c207a679614794a788d9fbc05f4549_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/d2c71b3a11b84034baaf1d280146e928_out.jpg",
        "model1": "InfEdit",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9703171356411447,
        "model2_overall_score": 0.6009446409858097
    },
    {
        "prompt": "change strawberries to oranges",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/91270b1f4cb64d57b92b4779a5e292cc_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/91270b1f4cb64d57b92b4779a5e292cc_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/58f7634dd1204798b4a46097a6d65990_out.jpg",
        "model1": "InfEdit",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.45940718610698483,
        "model2_overall_score": 0.19526335946273587
    },
    {
        "prompt": "change the toothpicks into candles",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/2043a1425c474f798f73910e1163ecd2_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/2043a1425c474f798f73910e1163ecd2_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f8d1737a63924955a100d9d4d2f6a07d_out.jpg",
        "model1": "MagicBrush",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8100398342073473,
        "model2_overall_score": 0.3821446989963668
    },
    {
        "prompt": "Remove the frosting between layers.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/91b49ec562c64b0ab51b3aea14057097_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/91b49ec562c64b0ab51b3aea14057097_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ed17a0ebfe9346699a0538bb2c55ef3e_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.12377989440832526,
        "model2_overall_score": 0.0813859754373285
    },
    {
        "prompt": "let the bed be wooden",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/486184edd3d94b7eb0d02408a6948f5c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/29db5ffc0dfd4781863daa03477ca3b4_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/486184edd3d94b7eb0d02408a6948f5c_out.jpg",
        "model1": "MagicBrush",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.05332720287898285,
        "model2_overall_score": 0.5632305957915015
    },
    {
        "prompt": "make the motorcycles and cars pink",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/e2450766aa6448f9a9c1bb4afd749635_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/e2450766aa6448f9a9c1bb4afd749635_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/429fee3424ce44148ab1013f277cd84a_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9536689096421073,
        "model2_overall_score": 0.08419317140259963
    },
    {
        "prompt": "Could he be in the forest?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/9ad1c5ac25934f03955ef923faf7b361_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/68c8159eee7a44768e514e7795221f9b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/9ad1c5ac25934f03955ef923faf7b361_out.jpg",
        "model1": "InfEdit",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.4906637255948628,
        "model2_overall_score": 0.40142824958412404
    },
    {
        "prompt": "let it be a bullet train",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/a2dcb09e4e0842bbb52d22aea8236cae_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/a2dcb09e4e0842bbb52d22aea8236cae_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6cf02f3844124b5fa05916bd6eba6da5_out.jpg",
        "model1": "SDEdit",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.07175411299732937,
        "model2_overall_score": 0.3851796187265931
    },
    {
        "prompt": "What if there was a drawing of a bird on his shirt?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/d4bf42722c704242a9aeef6db7bc720f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d4bf42722c704242a9aeef6db7bc720f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/4f6ff1eebc7a441382536d3b56e58cad_out.jpg",
        "model1": "MagicBrush",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.2548382389908015,
        "model2_overall_score": 0.43368898384769183
    },
    {
        "prompt": "change the green ball to blue",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/7880b240439c4d81a90fe2e5b546361c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ce945925b0b54e0d8a0efeab5e143f09_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/7880b240439c4d81a90fe2e5b546361c_out.jpg",
        "model1": "CosXLEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8570212906946612,
        "model2_overall_score": 0.9598279827257019
    },
    {
        "prompt": "let it be a bullet train",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/3404aaba6b814a5282c0dd3a68f8fb34_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/3feb447eebf34b2f8e79f4fa9d04ef33_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/3404aaba6b814a5282c0dd3a68f8fb34_out.jpg",
        "model1": "SDEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7117415525215566,
        "model2_overall_score": 0.7044950308099682
    },
    {
        "prompt": "let the woman have blonde hair",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/1ec83bd3c2704e69b2dcc986db167b1a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/1ec83bd3c2704e69b2dcc986db167b1a_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/a3de1e5c5464480483c179a97f4c9781_out.jpg",
        "model1": "MagicBrush",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.031271896007554534,
        "model2_overall_score": 0.09194766407545796
    },
    {
        "prompt": "let there be granite floor in the kitchen",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/b12e99c69bdf4ad6a1caa8ec04fcbc4f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/45447a1578a0428e986ac8430c04ceb0_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/b12e99c69bdf4ad6a1caa8ec04fcbc4f_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.21329143955312024,
        "model2_overall_score": 0.09076085077009699
    },
    {
        "prompt": "let the cat have blue eyes",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/cdcc78612b0f4da4a91d5d92465cbcca_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ed5dafc2c40e4d05b056feb74d640add_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/cdcc78612b0f4da4a91d5d92465cbcca_out.jpg",
        "model1": "MagicBrush",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9097253165051296,
        "model2_overall_score": 0.2573078277337023
    },
    {
        "prompt": "have the sun rise instead of set.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ae5c2af263a44a78990d8e6e86a0ef81_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/41f989f3007f4510bcd3c864a884372b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ae5c2af263a44a78990d8e6e86a0ef81_out.jpg",
        "model1": "CosXLEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8873396855276324,
        "model2_overall_score": 0.6285694387258363
    },
    {
        "prompt": "Make all the grass green.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/c53ced76375047e9b4c013982f73f6a4_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/c53ced76375047e9b4c013982f73f6a4_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/34d1d39ab94d4215900a0b59a434a4aa_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.01957395622309377,
        "model2_overall_score": 0.27469595614363784
    },
    {
        "prompt": "Get rid of the jar of cookies",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/b0e71923af9f4dd4a94e16b41b8f5f8f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/b0e71923af9f4dd4a94e16b41b8f5f8f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f4a782f8013f4ca7bc940e3ab4fff5e7_out.jpg",
        "model1": "MagicBrush",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.4105239125087603,
        "model2_overall_score": 0.1584855282758223
    },
    {
        "prompt": "He should be eating a watermelon",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/bfdec30a16fb4d049a9485919022f7df_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/bfdec30a16fb4d049a9485919022f7df_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/d2d8de672cfb4fe8a4657b94c2c4b1f0_out.jpg",
        "model1": "MagicBrush",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6653968242342876,
        "model2_overall_score": 0.44905820315006295
    },
    {
        "prompt": "let the cat have blue eyes",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/94768276dca24ff8becda91e61487166_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/745ea762ff8f4c678c672f854263ef40_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/94768276dca24ff8becda91e61487166_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5200819274296428,
        "model2_overall_score": 0.13873164067040045
    },
    {
        "prompt": "Make the apples green",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/2fd52e67c6e7466094662a33727ff74d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/2fd52e67c6e7466094662a33727ff74d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/8e616d5add22402794d03c3b714c843a_out.jpg",
        "model1": "PNP",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8000037211966033,
        "model2_overall_score": 0.3349188368072509
    },
    {
        "prompt": "give the girl a baseball bat instead of a racket",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/d2864b55d6054574906d552f2ac88809_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f0547fa3ee68431db9eb3abd0e932c24_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/d2864b55d6054574906d552f2ac88809_out.jpg",
        "model1": "CosXLEdit",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.0627655287287553,
        "model2_overall_score": 0.7238667733514449
    },
    {
        "prompt": "change the clock tower to a bell tower",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/afb241733bc64cb3a2b6f1e745f494d2_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/aa80cee9836241e39a281a7a6b61088d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/afb241733bc64cb3a2b6f1e745f494d2_out.jpg",
        "model1": "PNP",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9406590255238729,
        "model2_overall_score": 0.9867138494333089
    },
    {
        "prompt": "Let's add a cat on the roof.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f73e1a9bf19346d3a2c74302eef0cf9c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/fba777b84a3e49c9ac2305dfe062c790_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f73e1a9bf19346d3a2c74302eef0cf9c_out.jpg",
        "model1": "MagicBrush",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.45107886860799895,
        "model2_overall_score": 0.38072928828712294
    },
    {
        "prompt": "edit some mountains in the background",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/beb09507d08b483ebbfe357e427d7eaa_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f900bc4583f744ea9bb63b3c30b6d346_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/beb09507d08b483ebbfe357e427d7eaa_out.jpg",
        "model1": "CosXLEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9142046888841632,
        "model2_overall_score": 0.4459746162021181
    },
    {
        "prompt": "Make the donut a cupcake.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/7f5008e7eadf4139be0f12b098f88824_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/4bb6608fbd264d5986f491362483a162_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/7f5008e7eadf4139be0f12b098f88824_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.028624042593806354,
        "model2_overall_score": 0.0740695859114412
    },
    {
        "prompt": "Change the trees to palm trees",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/7301b0c486074995a96bd517ba5ec4c0_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7445059cc71f4077816a456f5baf67bd_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/7301b0c486074995a96bd517ba5ec4c0_out.jpg",
        "model1": "PNP",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.914537353960952,
        "model2_overall_score": 0.43542154709773184
    },
    {
        "prompt": "replace the chocolate with berries",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/6163fad054284098b148ccb6130d82a7_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/e519b00959614880bad81bae7f3745a3_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6163fad054284098b148ccb6130d82a7_out.jpg",
        "model1": "MagicBrush",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.17920223440156413,
        "model2_overall_score": 0.3835178174465671
    },
    {
        "prompt": "let the cat have blue eyes",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/41723c62dd274f24a6ea26af87d8d777_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/6e8a76bd44ef4d55a7c95e538fa5de4f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/41723c62dd274f24a6ea26af87d8d777_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.572636674458051,
        "model2_overall_score": 0.8759436921429631
    },
    {
        "prompt": "change the clock tower to a bell tower",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/54b50fc88234476fbf7cda948922b863_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/54b50fc88234476fbf7cda948922b863_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/66d4ac2207844cbe868c8c833fadf376_out.jpg",
        "model1": "PNP",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.21063711901716387,
        "model2_overall_score": 0.3204387824710129
    },
    {
        "prompt": "add a lizard on the counter",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/af56517640a445318c778c15eee13edf_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7ad5014fe4a34240b898d079e5222a08_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/af56517640a445318c778c15eee13edf_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6614261891840515,
        "model2_overall_score": 0.23277952961725423
    },
    {
        "prompt": "let the patch of flowers be daffodils",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/5f5d59f23a39474fa27b7e9d393f2e8b_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/5f5d59f23a39474fa27b7e9d393f2e8b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/c922b9866fdf452ba24459ca0b6ed083_out.jpg",
        "model1": "CosXLEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.45640933179367393,
        "model2_overall_score": 0.5012178487239759
    },
    {
        "prompt": "give the man glasses",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/b18c512b83e4437a96464cf1d18bb615_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/b18c512b83e4437a96464cf1d18bb615_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/b93502b999d748f2b8c42b671b3a1e62_out.jpg",
        "model1": "MagicBrush",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5929309905033622,
        "model2_overall_score": 0.9226452843644496
    },
    {
        "prompt": "change the truck into a taxi",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/d3ae480f12bd4770a2a54a4d05d3df22_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d3ae480f12bd4770a2a54a4d05d3df22_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/b8276506764f405692ab46fc3c6bf6b7_out.jpg",
        "model1": "CosXLEdit",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6394940888510133,
        "model2_overall_score": 0.7982186750284604
    },
    {
        "prompt": "remove the tennis ball",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/57fc4eba76c9456c9f0747e0869ae715_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/57fc4eba76c9456c9f0747e0869ae715_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/a78815cbd886427d95526634fc5bb831_out.jpg",
        "model1": "MagicBrush",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3412437757914244,
        "model2_overall_score": 0.7860839953500331
    },
    {
        "prompt": "Can we have mountains on the background?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/a64cca58b21943bda48b7a49ce716df6_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/9931cb127a304f3f8d30dd6c0e6750f7_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/a64cca58b21943bda48b7a49ce716df6_out.jpg",
        "model1": "CosXLEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6752418141251185,
        "model2_overall_score": 0.6309191498513901
    },
    {
        "prompt": "Let's add a dog next to the cows.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/a37458fe527246598148941fd88dd5cc_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/707e2059ea3a4156af84d6b4832e60a3_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/a37458fe527246598148941fd88dd5cc_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.79625174331231,
        "model2_overall_score": 0.35535473626843583
    },
    {
        "prompt": "Make the donut a cupcake.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/b01136c09f2741f39ac7b1a524753545_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7210490a1a8a43d4b932617f4e5f3de8_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/b01136c09f2741f39ac7b1a524753545_out.jpg",
        "model1": "SDEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.33799317610716995,
        "model2_overall_score": 0.06363445981970339
    },
    {
        "prompt": "Put a rat on the counter.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/86ea58ca6a19420fbf399f86d2036074_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/df0872b6a1d54cb4a753391e93eddeb9_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/86ea58ca6a19420fbf399f86d2036074_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.1091725221661306,
        "model2_overall_score": 0.6250624133830527
    },
    {
        "prompt": "Get rid of the jar of cookies",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ff0c23d7829a4a988f007950b83ccc69_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/b5d9522d9a60426095321b30572de4e9_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ff0c23d7829a4a988f007950b83ccc69_out.jpg",
        "model1": "MagicBrush",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7407303300725532,
        "model2_overall_score": 0.33210760750013013
    },
    {
        "prompt": "Add a giant wasp.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/279317aeff3d4caab5344467511a914e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f0dac56c1d454c609cc10d27482c7793_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/279317aeff3d4caab5344467511a914e_out.jpg",
        "model1": "MagicBrush",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.06424960377927691,
        "model2_overall_score": 0.665814332057895
    },
    {
        "prompt": "turn her hair white",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/90f05241b40c424ca9bfa8b52e43cc30_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/2128ca7692c449b4a8b403b6ba57927b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/90f05241b40c424ca9bfa8b52e43cc30_out.jpg",
        "model1": "InfEdit",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.18556963474355548,
        "model2_overall_score": 0.45499075837885405
    },
    {
        "prompt": "make the stop sing an animal sign",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/11a11760e2d94b65bd0788955a0f71b3_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/6388b5f2b8d24ee7b0ca813cafb0de7a_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/11a11760e2d94b65bd0788955a0f71b3_out.jpg",
        "model1": "CosXLEdit",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.33034442730999947,
        "model2_overall_score": 0.21298655094522112
    },
    {
        "prompt": "Make the piece of paper hanging on the wall a mirror",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/6a4dd8e6353447a0b322acf4e61007fe_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/c82e40ee38614f95aeb01008cda6b48a_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6a4dd8e6353447a0b322acf4e61007fe_out.jpg",
        "model1": "MagicBrush",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.2601432715105746,
        "model2_overall_score": 0.6823900702954996
    },
    {
        "prompt": "Put a wedding cake on one of the tables.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/4e05be375b8a44ebae54157c9a1a2ec7_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/4e05be375b8a44ebae54157c9a1a2ec7_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/52f283d22a2543c39069272c9a3ae61f_out.jpg",
        "model1": "MagicBrush",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6464759928853901,
        "model2_overall_score": 0.9019660096325232
    },
    {
        "prompt": "Add a giant wasp.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/45fa57e00c684dbdaf6670db88a9b844_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/45fa57e00c684dbdaf6670db88a9b844_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/0c6526e4668747d1b45a0fa2e796c10a_out.jpg",
        "model1": "CosXLEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.08523855958005522,
        "model2_overall_score": 0.7507652375036957
    },
    {
        "prompt": "There should be some cutlery on the table.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/613f822331d94f549aa6943e01619f0d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/613f822331d94f549aa6943e01619f0d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/44c19d87c37240479af9ab91c5fcfffb_out.jpg",
        "model1": "MagicBrush",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.10567334007103124,
        "model2_overall_score": 0.7595841638055659
    },
    {
        "prompt": "Make it a slice of pizza instead of the sandwich.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f75a4cbc4b224b798042a8cea75d3ab5_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/b4c81aa400ec4315b56a9ed25e02f901_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f75a4cbc4b224b798042a8cea75d3ab5_out.jpg",
        "model1": "InfEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.43680883147642136,
        "model2_overall_score": 0.3640573272182763
    },
    {
        "prompt": "add a woman inside the car",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/7b8a4cd4db7c469ea116183626ecdd3d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7b8a4cd4db7c469ea116183626ecdd3d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/e3e850519f164ddd8281126252a611f8_out.jpg",
        "model1": "CosXLEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6142378440767172,
        "model2_overall_score": 0.8357656901428402
    },
    {
        "prompt": "Change the woman's hair to blonde",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/a1876bd0e52e4546809da5391f744317_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/cc44963f166e483e9853cc1bafe93e67_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/a1876bd0e52e4546809da5391f744317_out.jpg",
        "model1": "InfEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7902025230524399,
        "model2_overall_score": 0.9851516193123452
    },
    {
        "prompt": "Make all the grass green.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f4e91db0849d473494db6dacea12a46d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f4e91db0849d473494db6dacea12a46d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/8f141b15a54f4c6e935c7911a9f1ee88_out.jpg",
        "model1": "InfEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.657850567991146,
        "model2_overall_score": 0.22267431138893712
    },
    {
        "prompt": "change the stuffed toys into rubber duckies",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f9527e68f1a844e085713a0f3f26287d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/6c2293bee9cd4a8e9525df572ff163fa_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f9527e68f1a844e085713a0f3f26287d_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5839525692675266,
        "model2_overall_score": 0.7757962277721846
    },
    {
        "prompt": "let the bed be wooden",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/3281f5e6230846d19b077fdf3dfe47be_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/3281f5e6230846d19b077fdf3dfe47be_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/7068059d31de4b0aa8d9380e702d91aa_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6787523921178524,
        "model2_overall_score": 0.23275708852848465
    },
    {
        "prompt": "replace the chocolate with berries",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/980a92a73abe4c108be73df6f803eab0_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/980a92a73abe4c108be73df6f803eab0_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/850df76eb9a849dd8c8996035feae962_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5775030609903147,
        "model2_overall_score": 0.8512996252688453
    },
    {
        "prompt": "add a bird on the road",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/80e15d93ff214fe8badce715908a070e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f14241501d484e67bfdcf8af363b29b4_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/80e15d93ff214fe8badce715908a070e_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9964408328917104,
        "model2_overall_score": 0.5698587589474252
    },
    {
        "prompt": "make two parasailers instead of one.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/2d69f22aec9540b790711780258886ec_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/8e2ed5fcc1ae4b898f155b8dc9a3793b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/2d69f22aec9540b790711780258886ec_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6617587362497057,
        "model2_overall_score": 0.8656490388124018
    },
    {
        "prompt": "let the woman cry",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/bfe74886ba6d48828105cb715451f1ac_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/bfe74886ba6d48828105cb715451f1ac_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/90a7f3fa30854bb1b0cb88e36caef41f_out.jpg",
        "model1": "MagicBrush",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5181585237448247,
        "model2_overall_score": 0.04930589682979181
    },
    {
        "prompt": "Add a cruise ship to the ocean.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/8f6c1e246b1f4b5d8047d5c0b4869f0d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/8f6c1e246b1f4b5d8047d5c0b4869f0d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/913ed14404c044f2abcf4cf7cead690d_out.jpg",
        "model1": "MagicBrush",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9328999013030151,
        "model2_overall_score": 0.8657432909360369
    },
    {
        "prompt": "replace the donuts with fruits",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/373e06832fce48a28135e5450da54faf_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/2156a7af6fa94ba7a86174ff1e837788_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/373e06832fce48a28135e5450da54faf_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3870117633626191,
        "model2_overall_score": 0.7766319336876065
    },
    {
        "prompt": "It could be a microwave next to the woman.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/44507427a3f24f77bf3a943b21cdfa76_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/094d946d5966466999760f9faee27b11_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/44507427a3f24f77bf3a943b21cdfa76_out.jpg",
        "model1": "MagicBrush",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.4231037920608198,
        "model2_overall_score": 0.753074006495862
    },
    {
        "prompt": "place a cat in the counter",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/7cce607ba281401db80c1bb26552a46c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/fa675bd177af4d95bba9e800e4a1e7b0_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/7cce607ba281401db80c1bb26552a46c_out.jpg",
        "model1": "CosXLEdit",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.4848965730314839,
        "model2_overall_score": 0.5477276035431404
    },
    {
        "prompt": "What if there was a drawing of a bird on his shirt?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/e1550cf2637d4191b6db4edd3fe8f9c0_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/613a4e589a524ec3ad4445d02eae5e3c_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/e1550cf2637d4191b6db4edd3fe8f9c0_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.968318560539687,
        "model2_overall_score": 0.7359674877312676
    },
    {
        "prompt": "change the bag of chips into a backpack",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/065dca10727b4bbeb01aa0fa31fededb_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/065dca10727b4bbeb01aa0fa31fededb_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/b2a4fc8d274c4e8a8fe8411fc447cc8a_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.42157026237264605,
        "model2_overall_score": 0.7375999554169514
    },
    {
        "prompt": "add flying eagles over the statue",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/413ac252687747c39646ab9878573710_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/46209fb58db24f759182eace177a1448_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/413ac252687747c39646ab9878573710_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6952335580392861,
        "model2_overall_score": 0.8310135105272296
    },
    {
        "prompt": "Can the man hold bananas too?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/4002698835914295807389c1f0fce847_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d27eec6ed1424cd5aa0f576cd72ea037_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/4002698835914295807389c1f0fce847_out.jpg",
        "model1": "PNP",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.45176343499195437,
        "model2_overall_score": 0.2322281491114292
    },
    {
        "prompt": "Let the monitor turn black.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/c126ade365c5484aad95507509f0b65f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/c126ade365c5484aad95507509f0b65f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f3e20d5876564a02b8c00b944fdc18e9_out.jpg",
        "model1": "PNP",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9532115958990125,
        "model2_overall_score": 0.7349963807425622
    },
    {
        "prompt": "let the plate contain ice cream",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/618f360e174a400389c43d8d97332219_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/4d903cc786f9431cbf6c787d64d8f9bc_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/618f360e174a400389c43d8d97332219_out.jpg",
        "model1": "MagicBrush",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.11983704538795958,
        "model2_overall_score": 0.5824557586518401
    },
    {
        "prompt": "Put a policeman in the intersection.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/465f24158d5042c98d21b3084c74af0a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/dbf804622a4545eeb60cd9b27e8cbb20_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/465f24158d5042c98d21b3084c74af0a_out.jpg",
        "model1": "InfEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.29385319838625523,
        "model2_overall_score": 0.7113865241445971
    },
    {
        "prompt": "add a lizard on the counter",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/e3c67d3e2b624af0a15035715941039d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/e3c67d3e2b624af0a15035715941039d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/7d43dac60a644db5be53b97d3ac297fe_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9382714497606974,
        "model2_overall_score": 0.7559019963744855
    },
    {
        "prompt": "Have there be a dolphin jumping out of the water",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ef30ed9278c444f6b392184a991ced0b_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ef30ed9278c444f6b392184a991ced0b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/5154f0e4a49d45cf94e67163a0c7c274_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.16140074745372068,
        "model2_overall_score": 0.17966793935045067
    },
    {
        "prompt": "Add a dear on the grass.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f549347ce47641c5b99a54915694ac59_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f549347ce47641c5b99a54915694ac59_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/12b3e8724cd24a308894606ac2307e8b_out.jpg",
        "model1": "CosXLEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.4832604310332076,
        "model2_overall_score": 0.8333977318310019
    },
    {
        "prompt": "Make all the grass green.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/216994a7253245e6b2336499a2262633_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/216994a7253245e6b2336499a2262633_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/bd1b95b4e2d64ee28c71dee333165d25_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8140890307393874,
        "model2_overall_score": 0.4471936726569081
    },
    {
        "prompt": "put a party hat on the dog",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/5ade395731b040a0a9bca2823a443ed5_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/622c6f4dac6a47e0bf492f114dc78940_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/5ade395731b040a0a9bca2823a443ed5_out.jpg",
        "model1": "CosXLEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.15183561514897304,
        "model2_overall_score": 0.9351477716021062
    },
    {
        "prompt": "make the motorcycles and cars pink",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/b9fd431feae74cbb91031998189c16a3_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/b9fd431feae74cbb91031998189c16a3_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/127a54989b0e4ba38370f627ec25e4fe_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8846278290501707,
        "model2_overall_score": 0.13185721192606137
    },
    {
        "prompt": "Let's add a monitor on the wall.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/c6a2259332c0426c85ee0f9b392ce7fd_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/c6a2259332c0426c85ee0f9b392ce7fd_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/b2496a7e26ed4df29459a3a433fe2811_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9856103378974644,
        "model2_overall_score": 0.9978184100061881
    },
    {
        "prompt": "place a penguin in the picture",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/71c19ad0348e4492a82af15ab67c4859_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/1768ada6c1da4e3ba50868e695aabf2d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/71c19ad0348e4492a82af15ab67c4859_out.jpg",
        "model1": "MagicBrush",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5637444960690599,
        "model2_overall_score": 0.8829231620326626
    },
    {
        "prompt": "give the girl a baseball bat instead of a racket",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ac60d0b6cba0470299ddd96df67ec58f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/33076e3d8d7d4bdfa153e49957d001d7_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ac60d0b6cba0470299ddd96df67ec58f_out.jpg",
        "model1": "PNP",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.014025424787881269,
        "model2_overall_score": 0.24011812878143812
    },
    {
        "prompt": "make it a pepperoni pizza",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/18634a8c073143509339143ef7cbf537_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/c8b29b973500462bac412ed6bb641cff_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/18634a8c073143509339143ef7cbf537_out.jpg",
        "model1": "PNP",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.13078066978173208,
        "model2_overall_score": 0.9737721810295619
    },
    {
        "prompt": "change the stop sign to a welcome sign",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/780895dffb444bad88685dda6624dcad_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/780895dffb444bad88685dda6624dcad_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/22a420333c1f45ac90a7c6240e1b0230_out.jpg",
        "model1": "CosXLEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8841998010161966,
        "model2_overall_score": 0.8385996925127828
    },
    {
        "prompt": "Make the man's pants all white.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/c100635cec3b488ca66332c94ae5d492_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/32e635636ff24e798943461aa7125b65_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/c100635cec3b488ca66332c94ae5d492_out.jpg",
        "model1": "CosXLEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.023983668457275176,
        "model2_overall_score": 0.5078750975982266
    },
    {
        "prompt": "change the teddy bear into a stuffed action figure toy",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/66e094be643f4a51935fc82c34c41616_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/3de7ccdfbfeb4ed7a5e24c4ea38be4aa_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/66e094be643f4a51935fc82c34c41616_out.jpg",
        "model1": "InfEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7293792339845482,
        "model2_overall_score": 0.4300702923823988
    },
    {
        "prompt": "Have the man be wearing a kilt",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/3499405c4ade406cb9f5ce72d4e5d9ce_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/8e99f06c53c540e3a6794fad0e551cfa_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/3499405c4ade406cb9f5ce72d4e5d9ce_out.jpg",
        "model1": "PNP",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5870738195635926,
        "model2_overall_score": 0.4504579689616106
    },
    {
        "prompt": "let a herd of sheep block the taxi",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/4c8977441df84c1fa585c86699b283a2_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/4c8977441df84c1fa585c86699b283a2_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6d4de1f5075e4df5a8660f1cc51fd51f_out.jpg",
        "model1": "PNP",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9486908253443069,
        "model2_overall_score": 0.45161524851924584
    },
    {
        "prompt": "put strawberry on the plate",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/8561007f5e5d4600904db1c58b39c94d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/42b8a0c288774c6990819fd99532b8c5_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/8561007f5e5d4600904db1c58b39c94d_out.jpg",
        "model1": "MagicBrush",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.720657185427955,
        "model2_overall_score": 0.1797851133714723
    },
    {
        "prompt": "change the Washington Monument with the Statue of Liberty",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/9084182295564e8abeaf24dc4f57bd2e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/9084182295564e8abeaf24dc4f57bd2e_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/0ca9a70362554f6dbe3c4574eb983466_out.jpg",
        "model1": "PNP",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.540617943860996,
        "model2_overall_score": 0.1360694920948735
    },
    {
        "prompt": "What if the child was holding a bottle of peper?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f80133b7f4d84cb4b2ee7d32eeb6876a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/144dff5ce07d4e2b9bf7d2a2a2e6a3ab_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f80133b7f4d84cb4b2ee7d32eeb6876a_out.jpg",
        "model1": "SDEdit",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.13726705303607645,
        "model2_overall_score": 0.5058878950382296
    },
    {
        "prompt": "change the color of the soccer ball.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/3314ff87645b4dd383afba5a94e4ab39_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/3314ff87645b4dd383afba5a94e4ab39_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/180206e4ca184cfd819c018443cf071f_out.jpg",
        "model1": "MagicBrush",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.13208264439342454,
        "model2_overall_score": 0.31676435771766287
    },
    {
        "prompt": "place a penguin in the picture",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/1f13fc4f4c3b4afa850272c2efb862da_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d72cf0f8a9f9460291e0ef755678f82d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/1f13fc4f4c3b4afa850272c2efb862da_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9035706782990554,
        "model2_overall_score": 0.25889726318160955
    },
    {
        "prompt": "get rid of the vase on top of the table",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/6229381196ad4a318fa03f0fe4bef272_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/6229381196ad4a318fa03f0fe4bef272_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/fb98f0c11d6e4129a8bb2b4e072dd41e_out.jpg",
        "model1": "Pix2PixZero",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.42274666497294544,
        "model2_overall_score": 0.4881893982562374
    },
    {
        "prompt": "replace the donuts with fruits",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/dc6ef9c1b19d4390a3cc2a0f990a9592_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/dc6ef9c1b19d4390a3cc2a0f990a9592_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ea62992047db4e39b41b6a7dc7812a4e_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7841374121987741,
        "model2_overall_score": 0.7922706211507156
    },
    {
        "prompt": "let a woman in a bridal gown stand near the cake",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/42cf1002a42444239e757958e83d6658_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/42cf1002a42444239e757958e83d6658_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/3fc12d6b6fef406e8229f67906d1cf42_out.jpg",
        "model1": "InfEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5418050386317583,
        "model2_overall_score": 0.9775314524427091
    },
    {
        "prompt": "Make the man's top blue.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ecdf17db81f247f9a9f8b0c662747973_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ecdf17db81f247f9a9f8b0c662747973_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/7a4066d610fb428a9033f7bd80aa3efb_out.jpg",
        "model1": "MagicBrush",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.38215142534475244,
        "model2_overall_score": 0.20312596830339114
    },
    {
        "prompt": "Remove the frosting between layers.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/4d22dd02d16a4418be6e1e28b6f16905_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/4d22dd02d16a4418be6e1e28b6f16905_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/0ff4319eeea745f3abcb6a0a33741d79_out.jpg",
        "model1": "PNP",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.26601957820699207,
        "model2_overall_score": 0.9137436212519059
    },
    {
        "prompt": "change the Washington Monument with the Statue of Liberty",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ced04e72f2284487982c240c7a7f80b6_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ced04e72f2284487982c240c7a7f80b6_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/5a759853f6f54314a98c0072920ca94f_out.jpg",
        "model1": "InfEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7644316676717694,
        "model2_overall_score": 0.01446312405797956
    },
    {
        "prompt": "change the color of the soccer ball.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/e8171a7dc9e34ad59b9b59491fbf132b_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/487a432263074c14bbbb91d6b440e90c_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/e8171a7dc9e34ad59b9b59491fbf132b_out.jpg",
        "model1": "MagicBrush",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6163295284291808,
        "model2_overall_score": 0.7534301453697442
    },
    {
        "prompt": "remove middle fruit and put a cat in place",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/88f5b2e1212a45ddad5ca687037c1470_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/203947c6acef410c8db495bab4bb8064_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/88f5b2e1212a45ddad5ca687037c1470_out.jpg",
        "model1": "SDEdit",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.19868119900083647,
        "model2_overall_score": 0.878365946011658
    },
    {
        "prompt": "place a cat in the counter",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/4e9828f45a57463e957bbefd419e2249_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/b4e243bf201c439892e2859b99638189_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/4e9828f45a57463e957bbefd419e2249_out.jpg",
        "model1": "InfEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.20012184668849742,
        "model2_overall_score": 0.7752339397902491
    },
    {
        "prompt": "Have one of the children be blowing bubbles",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/b2c9325eb04546cfaa051156c4290ed0_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/b3ce0fe05aa7415d861ecfc1bccc98a4_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/b2c9325eb04546cfaa051156c4290ed0_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.35678544539015955,
        "model2_overall_score": 0.7877356268389359
    },
    {
        "prompt": "remove the tent and add a bonfire",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/15f9ab2680e0467b8f4a5d8e80b6152d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/e68021a90b884bc8b2741d37c92993c1_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/15f9ab2680e0467b8f4a5d8e80b6152d_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6524997291061619,
        "model2_overall_score": 0.8190362550816397
    },
    {
        "prompt": "Let the bluebery cake be topped with chocolate syrup.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/14c09c46f0084e988c30d6ee0b3a2c1a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/4333a5af979043fbb354bce314e67452_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/14c09c46f0084e988c30d6ee0b3a2c1a_out.jpg",
        "model1": "CosXLEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8192238128208692,
        "model2_overall_score": 0.24824962528407402
    },
    {
        "prompt": "turn the remote into a pizza",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/0d312d6d130644828653689e0c131d22_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/0d312d6d130644828653689e0c131d22_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/69d6a1f6494f4975a5123cab8446cc58_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9102215381210814,
        "model2_overall_score": 0.9474196383188922
    },
    {
        "prompt": "Add a cruise ship to the ocean.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/e1e385b1b24746b89bda34bc2a400b28_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f6d75955d99a4765aa40a87698408311_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/e1e385b1b24746b89bda34bc2a400b28_out.jpg",
        "model1": "CosXLEdit",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6390336014295663,
        "model2_overall_score": 0.1351014239083923
    },
    {
        "prompt": "change the Washington Monument with the Statue of Liberty",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/6a5db305d9f04f4b8599af603f968c9c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/6a5db305d9f04f4b8599af603f968c9c_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/a30b54f213db4ed588e404781dd66b17_out.jpg",
        "model1": "CosXLEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.10995763557093707,
        "model2_overall_score": 0.7847608104201487
    },
    {
        "prompt": "replace the donuts with fruits",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/b0d70876b79049298d24b8792e72660b_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/fbaf767d575a44b5a70c24b2aac305ed_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/b0d70876b79049298d24b8792e72660b_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.1665209367550149,
        "model2_overall_score": 0.6240939761500663
    },
    {
        "prompt": "change the Washington Monument with the Statue of Liberty",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/7552e8bc48d149fd8c7dbe0ea5608420_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7552e8bc48d149fd8c7dbe0ea5608420_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/26978d553b884636a9e8e313a2ccbf44_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6193046742546889,
        "model2_overall_score": 0.6965838426104378
    },
    {
        "prompt": "Remove the salad on the side.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/05d7c1a758ed4899a34db358581267a4_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/05d7c1a758ed4899a34db358581267a4_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/7034b00345d24b8ea27403a004cecd41_out.jpg",
        "model1": "Pix2PixZero",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5128861385674924,
        "model2_overall_score": 0.2880369584083605
    },
    {
        "prompt": "Put popcorn in the plate.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/c9afe4ce94d0411eb12a580762ad79dc_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/359339043f3846b4aa1b0a8d5aa901fb_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/c9afe4ce94d0411eb12a580762ad79dc_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.562438195285388,
        "model2_overall_score": 0.8654661350499795
    },
    {
        "prompt": "change the cat to a fox.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/8aef6c04ba4449c599f5eed962f52d48_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/b7f9cf8230064348adf044d6e3d0a08c_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/8aef6c04ba4449c599f5eed962f52d48_out.jpg",
        "model1": "InfEdit",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6399654800324928,
        "model2_overall_score": 0.10204454547760844
    },
    {
        "prompt": "remove middle fruit and put a cat in place",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/9b2c2caf9e5b4e8d9a1a00ef6f5ecbc0_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7de51cfe878e47d18fba26aa47aa0231_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/9b2c2caf9e5b4e8d9a1a00ef6f5ecbc0_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9423690733034139,
        "model2_overall_score": 0.04506922501016419
    },
    {
        "prompt": "Make it a black sheep.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/58161cf0691d4f04a4159d834693918c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/9c6f6cbdfba5412eaeb87695b9051388_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/58161cf0691d4f04a4159d834693918c_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.37713573509267895,
        "model2_overall_score": 0.9598440012977769
    },
    {
        "prompt": "Put a policeman in the intersection.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/b4b886c4ffe24ef584b9bfc0d9eb4f51_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/b4b886c4ffe24ef584b9bfc0d9eb4f51_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/eab0f355ee5f49838c68521fb29ce609_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.30393753997329687,
        "model2_overall_score": 0.1769608536838867
    },
    {
        "prompt": "add a car instead of motorcycles",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/10adc0f89b0d42dba85b11a45e4f9520_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/e30c6a79eeb043d7baf6d14d90abb029_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/10adc0f89b0d42dba85b11a45e4f9520_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7582829007666797,
        "model2_overall_score": 0.05502314825377663
    },
    {
        "prompt": "let the tree be conifers",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/e27b285281644c129a858cc8c140bd77_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/50005107f8754838a2c13fe8cd143ca4_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/e27b285281644c129a858cc8c140bd77_out.jpg",
        "model1": "MagicBrush",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.0515797202971463,
        "model2_overall_score": 0.9933164283604491
    },
    {
        "prompt": "He should be eating a watermelon",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/cbbfc2363368431681ea779f073f9996_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/cbbfc2363368431681ea779f073f9996_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/fc3a189562194d98881ac65db661a176_out.jpg",
        "model1": "CosXLEdit",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.28202629822086367,
        "model2_overall_score": 0.23017145539129558
    },
    {
        "prompt": "put a robot tiger next to the bear",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/1fbf48aa0fd54e6686df2ea4137a2d8f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/1fbf48aa0fd54e6686df2ea4137a2d8f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/16e4364a62bd4631b3a14bf402532465_out.jpg",
        "model1": "CosXLEdit",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.08607506604548165,
        "model2_overall_score": 0.6502910514358088
    },
    {
        "prompt": "Change the trees to palm trees",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ced28c188c5d40ea8afdd738f2e5d3ef_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ced28c188c5d40ea8afdd738f2e5d3ef_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6f1bb0df5eb444849762c0b778206f9a_out.jpg",
        "model1": "CosXLEdit",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.10844646208774344,
        "model2_overall_score": 0.1774120331698309
    },
    {
        "prompt": "Add a cup of coffee.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/1a5d93b543ad4bc1806dc8f4456b3713_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/1a5d93b543ad4bc1806dc8f4456b3713_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/8177eaf9c41b4703acfc87ca66beb8a0_out.jpg",
        "model1": "SDEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7726846572163076,
        "model2_overall_score": 0.1507019160790205
    },
    {
        "prompt": "put a lion in the place of the donkey",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/5ef6363511424a2ba34d163c5a15cd24_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ff7b67cca69d4f16abe0bf193fa6c437_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/5ef6363511424a2ba34d163c5a15cd24_out.jpg",
        "model1": "PNP",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.2990194886266364,
        "model2_overall_score": 0.16425605244141783
    },
    {
        "prompt": "make the ground a forest instead of a slope.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/545ffb2a58dd436b93d1a86816556c6b_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/b60d35ab66fa45b5ae905e6b02efa231_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/545ffb2a58dd436b93d1a86816556c6b_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6257988410381795,
        "model2_overall_score": 0.9234517260759644
    },
    {
        "prompt": "let it be a bullet train",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/8285c38ae97b49fe91a509dfbbdb12bd_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7ba2e602f76a4b3bb215ee358e8e5f91_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/8285c38ae97b49fe91a509dfbbdb12bd_out.jpg",
        "model1": "Pix2PixZero",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.2587740594762482,
        "model2_overall_score": 0.1532696154835259
    },
    {
        "prompt": "add a plane in the sky",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/32dec689229c435da6dd65ff71b07f6b_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/3723644742294bb1a8803ef6db6741ed_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/32dec689229c435da6dd65ff71b07f6b_out.jpg",
        "model1": "CosXLEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.2881765920460101,
        "model2_overall_score": 0.3758460221516343
    },
    {
        "prompt": "Replace the greens with onions.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/42969aa9088b4c3ea78dba87158d0c8c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/1932f9e8caa8476a86236904a1bda2fc_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/42969aa9088b4c3ea78dba87158d0c8c_out.jpg",
        "model1": "InfEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9535731992985835,
        "model2_overall_score": 0.6236212515978931
    },
    {
        "prompt": "make the people angry",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/c6ed141e6a5641f6a9fe1cf5f7289d53_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/c6ed141e6a5641f6a9fe1cf5f7289d53_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/913c92bf601142c196c21f12c099ece5_out.jpg",
        "model1": "MagicBrush",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.4150281741534452,
        "model2_overall_score": 0.28959671186256863
    },
    {
        "prompt": "Make the man's top blue.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/5d70b617b6154234b030cc5c13026ba1_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/5d70b617b6154234b030cc5c13026ba1_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/cd8e3a22997f44cc81ad675a98b6ea60_out.jpg",
        "model1": "MagicBrush",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.4007071322972965,
        "model2_overall_score": 0.05124932908895785
    },
    {
        "prompt": "add a rocket in the sky",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/fbd5209d9c7447738fb79a6d031ded17_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/bcbc44f54b1e4bbb88ee708386623d9f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/fbd5209d9c7447738fb79a6d031ded17_out.jpg",
        "model1": "CosXLEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6128907281208866,
        "model2_overall_score": 0.38925076943599357
    },
    {
        "prompt": "Put a show about cats on the TV.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/3ce38a85abfc4effb9a9f7f7c8a13d95_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7e27d348a3294805858ea3b3188cc603_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/3ce38a85abfc4effb9a9f7f7c8a13d95_out.jpg",
        "model1": "InfEdit",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3753489273377517,
        "model2_overall_score": 0.7639396613574168
    },
    {
        "prompt": "change the stuffed toys into rubber duckies",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/deac790262724587a867cf6f39dd374d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/deac790262724587a867cf6f39dd374d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/98f81c41cd06422ea43af2dca663ad63_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.06295063482015051,
        "model2_overall_score": 0.6891405763887467
    },
    {
        "prompt": "Put a wooden floor on the kitchen.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/38ce747e96ff456d855cf931d42dc18e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/38ce747e96ff456d855cf931d42dc18e_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/231bdb3245b546efba8b5dbeec1d424e_out.jpg",
        "model1": "MagicBrush",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8654952532413489,
        "model2_overall_score": 0.13656841407087872
    },
    {
        "prompt": "change the stop sign to a welcome sign",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/b4902c48a0a249f5a5c5619e576001f3_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/73e44f7fddc84315a00a59db2332e505_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/b4902c48a0a249f5a5c5619e576001f3_out.jpg",
        "model1": "InfEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8409398597548066,
        "model2_overall_score": 0.1531898805625308
    },
    {
        "prompt": "Make the man's top blue.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/dad40d629e5b4daca7a5c6a9c57b7dd6_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/dad40d629e5b4daca7a5c6a9c57b7dd6_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/846082a0ce28418eb75c0a8f8f1f282d_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7250179062749954,
        "model2_overall_score": 0.6847153569182274
    },
    {
        "prompt": "Make the apples green",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f582a32655634dbabf3e60057a2bd5bc_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f582a32655634dbabf3e60057a2bd5bc_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/8d4139eaabef49acb441a4243ca56016_out.jpg",
        "model1": "InfEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.2512921612035347,
        "model2_overall_score": 0.6732400701442024
    },
    {
        "prompt": "add a polar bear",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/35fa9dcea0c44dc487eab8b62a9a9dc5_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/fb8b964a1a8c4daeb9ce30029a8d05f7_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/35fa9dcea0c44dc487eab8b62a9a9dc5_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.21476211023765446,
        "model2_overall_score": 0.566946486494871
    },
    {
        "prompt": "change the toothpicks into candles",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/eca0eeae1d3f4214acb7497fc30fd89c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/eca0eeae1d3f4214acb7497fc30fd89c_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/d1996fb6db184f6b887c2575209d6c1c_out.jpg",
        "model1": "MagicBrush",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6091265834032323,
        "model2_overall_score": 0.384437495145711
    },
    {
        "prompt": "put a lion in the place of the donkey",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/aec289567ec34c61baeccd7ab2a45bd7_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/272430ec5abb4821b6b3d180868d5542_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/aec289567ec34c61baeccd7ab2a45bd7_out.jpg",
        "model1": "PNP",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.06427830001994095,
        "model2_overall_score": 0.1891754255828605
    },
    {
        "prompt": "let the plate contain ice cream",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/b7a6392532a64ec59be381b00f063d4f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/1e301eb494f44225a8b4f070df9637c8_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/b7a6392532a64ec59be381b00f063d4f_out.jpg",
        "model1": "InfEdit",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.853951562195562,
        "model2_overall_score": 0.11797611381392048
    },
    {
        "prompt": "replace the donuts with fruits",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/02d1b74f60924f08b8b116c078896751_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f4b83f33f2b74aa8834bbdbe4627d724_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/02d1b74f60924f08b8b116c078896751_out.jpg",
        "model1": "PNP",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9179551229086371,
        "model2_overall_score": 0.9007624167054415
    },
    {
        "prompt": "let a woman in a bridal gown stand near the cake",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/bbe4910536424d39966c97831978cd85_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/bbe4910536424d39966c97831978cd85_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/c5d5c1c0112c43cab721bbf3efe917db_out.jpg",
        "model1": "SDEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8058083485687699,
        "model2_overall_score": 0.7608208221546394
    },
    {
        "prompt": "add a dog barking near shore",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f5d8b1b214204920a8e815e96af65070_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f5d8b1b214204920a8e815e96af65070_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/8616319d7b694c13ae21b73fcfb90c08_out.jpg",
        "model1": "MagicBrush",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5592143291588144,
        "model2_overall_score": 0.7853077410865407
    },
    {
        "prompt": "Can we have mountains on the background?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/48b54d41a68043d0bec569d88c6fe15d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/bbd4e597a06c495a957965dca8b2023a_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/48b54d41a68043d0bec569d88c6fe15d_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7733839659296219,
        "model2_overall_score": 0.4928552965976527
    },
    {
        "prompt": "Remove the frisbee.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f1eac837f5d7416db25e3e4678522763_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/13e2d58392804c35a34b9c74f5b6b390_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f1eac837f5d7416db25e3e4678522763_out.jpg",
        "model1": "MagicBrush",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.2891593578541968,
        "model2_overall_score": 0.10058573549149774
    },
    {
        "prompt": "change the cat to a fox.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/32b25979629f49d983dc1c328d64c579_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/32b25979629f49d983dc1c328d64c579_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6931a71783774e1b8162a8b6151bacc3_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9374734353458348,
        "model2_overall_score": 0.6584244337009179
    },
    {
        "prompt": "There should be some cutlery on the table.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/6694f7933e7a43cd8298b62a524c6701_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/6694f7933e7a43cd8298b62a524c6701_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/00f87181c8924a8b8123666b70b32171_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.05057285481981688,
        "model2_overall_score": 0.2612140960494592
    },
    {
        "prompt": "turn the frisbee into a soccer ball",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/fc3962bb5e8249acb5dc0b40abae762e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f93f063a437649fdadb4840f1f55e632_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/fc3962bb5e8249acb5dc0b40abae762e_out.jpg",
        "model1": "MagicBrush",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.05870512633786262,
        "model2_overall_score": 0.012387295803640841
    },
    {
        "prompt": "replace the donuts with fruits",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f133929dc6f44f2c93aa33e1712cff10_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/91244d153f9a499bb1a4cddd0cb0be02_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f133929dc6f44f2c93aa33e1712cff10_out.jpg",
        "model1": "InfEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7984979716102749,
        "model2_overall_score": 0.04362592632059725
    },
    {
        "prompt": "Put a policeman in the intersection.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ca5e7891e6124f3aa4d18e75e948990d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/1cdfd0c727b646dd833856386d65c1f7_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ca5e7891e6124f3aa4d18e75e948990d_out.jpg",
        "model1": "InfEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8406461124451262,
        "model2_overall_score": 0.7537500053657077
    },
    {
        "prompt": "Remove the frosting between layers.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/970862afac7548d3b222bc40907836db_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/dbb12da44e124580a6109924084df344_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/970862afac7548d3b222bc40907836db_out.jpg",
        "model1": "CosXLEdit",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8998800929377699,
        "model2_overall_score": 0.8698192901023423
    },
    {
        "prompt": "let there be granite floor in the kitchen",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/c161d051bbcd477b848af2156f4c36f1_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/c161d051bbcd477b848af2156f4c36f1_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/de25ad0cae76486ca4368cb3c8bb0995_out.jpg",
        "model1": "InfEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6846641535586301,
        "model2_overall_score": 0.49943422440507546
    },
    {
        "prompt": "have the sun rise instead of set.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/5e118599701a485ba92d8c7a25a7622b_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/5602c2abf33342abb049936fabef5365_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/5e118599701a485ba92d8c7a25a7622b_out.jpg",
        "model1": "CosXLEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.773969438533799,
        "model2_overall_score": 0.09287495088937325
    },
    {
        "prompt": "have the sun rise instead of set.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/62f3b4d9a062463a9e253c8daaa4b199_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/593d431e1ee4467ca9dde250b9ab4f00_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/62f3b4d9a062463a9e253c8daaa4b199_out.jpg",
        "model1": "MagicBrush",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.22161996226201086,
        "model2_overall_score": 0.8036777592972316
    },
    {
        "prompt": "remove the tennis ball",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/430ec2c769754c6d991141a41bcafaf0_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/430ec2c769754c6d991141a41bcafaf0_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/206bfa4103264b52bb88732a007d5b4f_out.jpg",
        "model1": "InfEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.757878804838356,
        "model2_overall_score": 0.09206875695830552
    },
    {
        "prompt": "Have the cow wear a hat.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/b08d3dd5a3984d61bfe452b38d046681_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/b08d3dd5a3984d61bfe452b38d046681_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/bd5b4e02521a467da8769fc4ce8d0910_out.jpg",
        "model1": "CosXLEdit",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3178626205075248,
        "model2_overall_score": 0.9051747619951708
    },
    {
        "prompt": "let the patch of flowers be daffodils",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/e5765210358643e6b58df0df8550de4b_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/e5765210358643e6b58df0df8550de4b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/2c6153b31c8d4880b6ab95de7c801fad_out.jpg",
        "model1": "InfEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5276344774771954,
        "model2_overall_score": 0.4575468903464808
    },
    {
        "prompt": "let the bed be wooden",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/2494e7004c7346528ceb6c08d0b7b4a0_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7c0c8c0c1e3f405ab376ef2eb8295b41_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/2494e7004c7346528ceb6c08d0b7b4a0_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7433718097589964,
        "model2_overall_score": 0.6942045425843287
    },
    {
        "prompt": "The bed should be red.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/603b0e6cd92d4954a3e3018df2545545_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/603b0e6cd92d4954a3e3018df2545545_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/df8ff636beda4cebbd384bb6999830db_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.33992742958994104,
        "model2_overall_score": 0.6067810637895832
    },
    {
        "prompt": "place a cat in the counter",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/27390329f8964ff886b271af9c4bab00_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/27390329f8964ff886b271af9c4bab00_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/2437d45090024667aa120e52c324f170_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8645075111596118,
        "model2_overall_score": 0.5840234364821352
    },
    {
        "prompt": "make the cat's nose black",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/1ce654d46d9f45e4a329d6e756d4515e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/1ce654d46d9f45e4a329d6e756d4515e_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ab9575c15c8f4d548754178194a8d974_out.jpg",
        "model1": "CosXLEdit",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8548507207932574,
        "model2_overall_score": 0.20201065991012013
    },
    {
        "prompt": "add a plane in the sky",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/9b084608728c46d2ae9f777df5f08090_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/9b084608728c46d2ae9f777df5f08090_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/968a8ec9447c4a5e90050c7128d1ef50_out.jpg",
        "model1": "CosXLEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.33728974639015996,
        "model2_overall_score": 0.6344397124944415
    },
    {
        "prompt": "Put a pond next to the cows.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ca4abb8e99fc407f8b893972d57f9fbf_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ca4abb8e99fc407f8b893972d57f9fbf_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/954c38428ffb4a7f8ee1903857d43fef_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.017911019483805357,
        "model2_overall_score": 0.8387714360813562
    },
    {
        "prompt": "let a herd of sheep block the taxi",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/7d29e0ccdaf947aead039c4946632b44_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/8d39922d51374da08a318bf8b8a48dad_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/7d29e0ccdaf947aead039c4946632b44_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5538551305492252,
        "model2_overall_score": 0.44522361565174107
    },
    {
        "prompt": "let there be potted plant",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/36fc0430bc714ee4868ab4e33874dc59_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/36fc0430bc714ee4868ab4e33874dc59_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/cfb450f2d5e745d793244f64c8d79dcb_out.jpg",
        "model1": "InfEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.1382528437420465,
        "model2_overall_score": 0.4524136066767712
    },
    {
        "prompt": "Make the zebra a regular horse.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/6263abdd4e814f26b5148dbdf41f086f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/6263abdd4e814f26b5148dbdf41f086f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/d027ca56f6604b24a5311a906825b0f5_out.jpg",
        "model1": "InfEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6602455643329521,
        "model2_overall_score": 0.7025302079329185
    },
    {
        "prompt": "make the stop sing an animal sign",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/18ba731c4d1f4bf39c1aa432d1c43ccc_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/8eec6fe148f24ea6b0ba599933827d74_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/18ba731c4d1f4bf39c1aa432d1c43ccc_out.jpg",
        "model1": "CosXLEdit",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9766982646467351,
        "model2_overall_score": 0.3038091396346375
    },
    {
        "prompt": "Put a pond next to the elephant",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/e5430e75f1bb44db926be686901522f2_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/e5430e75f1bb44db926be686901522f2_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/3616c194a443439894c4d557b85ae55a_out.jpg",
        "model1": "PNP",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9088379105531741,
        "model2_overall_score": 0.6718537844981483
    },
    {
        "prompt": "Replace the greens with onions.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/caa80706e1fb4179a791be95945adaed_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/caa80706e1fb4179a791be95945adaed_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/76a7d965bc6f4e51980ee0c08a6a0f0c_out.jpg",
        "model1": "InfEdit",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.48942688677225654,
        "model2_overall_score": 0.8939772622765295
    },
    {
        "prompt": "let the cat wear a bow tie instead of a tie",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/9355a6582b73403ab67bfb7f2bc1a0cb_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/9355a6582b73403ab67bfb7f2bc1a0cb_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/26580b3adc8640d2982246256cfed72a_out.jpg",
        "model1": "InfEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.33997934475412306,
        "model2_overall_score": 0.3157644745276318
    },
    {
        "prompt": "change the toothpicks into candles",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/aab69b3fe4e146e68e236eca3e514921_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/aab69b3fe4e146e68e236eca3e514921_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/e2cd51f9f0ef4f66abb7a59d29336744_out.jpg",
        "model1": "InfEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9409788476843107,
        "model2_overall_score": 0.7175618533932785
    },
    {
        "prompt": "edit some mountains in the background",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/4a9fc1de0ae54fe3bc175cd736e14d6e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/91b55aba2abf4085adee7b7865df42ed_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/4a9fc1de0ae54fe3bc175cd736e14d6e_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5028386427570907,
        "model2_overall_score": 0.6264427685785808
    },
    {
        "prompt": "give her a baseball cap",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/a0006199aa3a4cc096a13aaa90767654_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/a0006199aa3a4cc096a13aaa90767654_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/9827a227582140b8b0592ca0241f5ef6_out.jpg",
        "model1": "MagicBrush",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7292140912544856,
        "model2_overall_score": 0.0602436906723105
    },
    {
        "prompt": "give her a skirt to wear",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/bc49a42989754354b855611d52477cd1_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/bc49a42989754354b855611d52477cd1_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/2f7c79f5cd104723a7c9b4265d2364ad_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.0075706514811448855,
        "model2_overall_score": 0.8921632216798572
    },
    {
        "prompt": "Let the bluebery cake be topped with chocolate syrup.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/7ce7854bfbb54481a8fd5b11c6268f8e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d8507cce1b5a4ebe9529706c56124602_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/7ce7854bfbb54481a8fd5b11c6268f8e_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5263521114808921,
        "model2_overall_score": 0.2071001041822541
    },
    {
        "prompt": "He should be eating a watermelon",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/0459049cad724afc909b00ec9309a83c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/0459049cad724afc909b00ec9309a83c_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/5320572c73e3487f9a8ebba3be5b0c0c_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7451377195027831,
        "model2_overall_score": 0.8921625125923194
    },
    {
        "prompt": "Add a dear on the grass.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/09d18427aea64de291b2c92b023a2fcd_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f2165741deed4f14b03381bdf5f8a968_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/09d18427aea64de291b2c92b023a2fcd_out.jpg",
        "model1": "CosXLEdit",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.09423959208312571,
        "model2_overall_score": 0.4981206296615879
    },
    {
        "prompt": "Replace the coffee with beer.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/d4c8363b099f46f2a25002136b3555ff_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d4c8363b099f46f2a25002136b3555ff_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/c719bf567e4a4d73ac0194d25e90b739_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.09061786615913581,
        "model2_overall_score": 0.16041951843470503
    },
    {
        "prompt": "let it be a single sink",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/37a75f1891a7466d8bb88c42ea8c7877_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/c0fe3d80b4d0476fa723a36fcec5f81f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/37a75f1891a7466d8bb88c42ea8c7877_out.jpg",
        "model1": "Pix2PixZero",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.512114986392315,
        "model2_overall_score": 0.032109567593551325
    },
    {
        "prompt": "let a woman in a bridal gown stand near the cake",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/edd644292bb04cbb9e0264a72f0349f4_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d1c09a30c9a541f9abf5d593e8ae013f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/edd644292bb04cbb9e0264a72f0349f4_out.jpg",
        "model1": "InfEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.4691439421756234,
        "model2_overall_score": 0.38720381176266216
    },
    {
        "prompt": "make the woman hold a camera",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/745da0b616694c1d82a1ca6be97669f2_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/3ea91618a60d4f038f9e4d8856fe5c1b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/745da0b616694c1d82a1ca6be97669f2_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.32485500404713696,
        "model2_overall_score": 0.08475006363945714
    },
    {
        "prompt": "Remove the salad on the side.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/3c5767db9dfd4ada9a218916195ed20a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/8c6600abfc3b48318ad0e3387596c944_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/3c5767db9dfd4ada9a218916195ed20a_out.jpg",
        "model1": "PNP",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9551031989640354,
        "model2_overall_score": 0.2695900965249759
    },
    {
        "prompt": "let the woman have blonde hair",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/fdc3c6ee559a4a43aedfc397624740d6_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/fdc3c6ee559a4a43aedfc397624740d6_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/94895f53eb39414495de3b7befea9675_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5204280849949843,
        "model2_overall_score": 0.7974788985977203
    },
    {
        "prompt": "make the ramp cement",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/4bce20e803154f0696f92a6b2bce15e6_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/b2437559eef8427588b0273e0f9a32c5_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/4bce20e803154f0696f92a6b2bce15e6_out.jpg",
        "model1": "InfEdit",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.17513992231875763,
        "model2_overall_score": 0.5241907335423677
    },
    {
        "prompt": "Make the cake a chocolate cake",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/dca524f6814247cba3763d3156e40822_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/dca524f6814247cba3763d3156e40822_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/578457648cd9436cba6dd0bf36969d03_out.jpg",
        "model1": "MagicBrush",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8102680223589508,
        "model2_overall_score": 0.0819644493125764
    },
    {
        "prompt": "What if the man had a hat?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f2fd3a9117624f5286add3d66eb82805_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/19ae4a474f264ea5b03d75e46c40936e_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f2fd3a9117624f5286add3d66eb82805_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.279334221418024,
        "model2_overall_score": 0.49564969834016526
    },
    {
        "prompt": "make the cat's nose black",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/424dc91262934af094432c6493f88f3d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/c998bd9d84134935a85b37beec20054a_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/424dc91262934af094432c6493f88f3d_out.jpg",
        "model1": "MagicBrush",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8780122916503404,
        "model2_overall_score": 0.2765393210534405
    },
    {
        "prompt": "change the double deck bus to a truck",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f0a67e84ae6f421ca05b0c2580563d26_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f0a67e84ae6f421ca05b0c2580563d26_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/3320500989754accaa2034f850db5419_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3137551296652372,
        "model2_overall_score": 0.7500516540761685
    },
    {
        "prompt": "let the patch of flowers be daffodils",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/25bee2a6c57c427a8331b2dd66a92701_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/25bee2a6c57c427a8331b2dd66a92701_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/934a30e5c7b7445189a98e26e49b1cf4_out.jpg",
        "model1": "InfEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6742411390098579,
        "model2_overall_score": 0.04720609655971997
    },
    {
        "prompt": "remove the tennis ball",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/06b7c9d6cbb34862ba6611d10b90ad91_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/06b7c9d6cbb34862ba6611d10b90ad91_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/de00a6c1e9ea4ee9bfada0a5715c75a4_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.30748756493867513,
        "model2_overall_score": 0.37102799809903253
    },
    {
        "prompt": "Can we have mountains on the background?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/cfdbbee6b8ca449f835a173d29d9cd26_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d70df732b5104b53acc89aaa08a42b8c_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/cfdbbee6b8ca449f835a173d29d9cd26_out.jpg",
        "model1": "MagicBrush",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7375375248636359,
        "model2_overall_score": 0.49969591350729414
    },
    {
        "prompt": "let the dog have a newspaper in its mouth",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/726c046008d0400c8e78d7f5283117af_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ecf9a61b02714810b9f533e4e2e21d2a_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/726c046008d0400c8e78d7f5283117af_out.jpg",
        "model1": "SDEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.15090954848496008,
        "model2_overall_score": 0.8701354580928271
    },
    {
        "prompt": "put strawberry on the plate",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/10b7e52bc711447b8d817cbbf87925e6_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/82e7599a57b447f48c9db611455e1e11_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/10b7e52bc711447b8d817cbbf87925e6_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7642437603447586,
        "model2_overall_score": 0.9710867099051392
    },
    {
        "prompt": "add a lizard on the counter",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/c8b04a664a6d4e658a420225ea4ed7a2_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/8c37bf96950f407c8bef326975fb2634_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/c8b04a664a6d4e658a420225ea4ed7a2_out.jpg",
        "model1": "SDEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8064020727593528,
        "model2_overall_score": 0.6184168256358231
    },
    {
        "prompt": "change the trees at the back into palm trees",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/dc276362223146b0b45dbbb3eb153a8b_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/9d772b203a3345d5a1cfa83372be4180_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/dc276362223146b0b45dbbb3eb153a8b_out.jpg",
        "model1": "CosXLEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6422075376336052,
        "model2_overall_score": 0.8148082948014325
    },
    {
        "prompt": "change the stuffed toys into rubber duckies",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/cafaed8532b741ae9f7f2423aea1b64d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/cafaed8532b741ae9f7f2423aea1b64d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/76103fd2e52642268ac4d47d91e1ce6b_out.jpg",
        "model1": "PNP",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9972812355469731,
        "model2_overall_score": 0.716228689975432
    },
    {
        "prompt": "Make the dog's eyes closed.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/84085a23a5bd4be1a958bafcf555552d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/84085a23a5bd4be1a958bafcf555552d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6bc4f25a87704f099839fd21c431be35_out.jpg",
        "model1": "InfEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7348651181135208,
        "model2_overall_score": 0.6011924961828689
    },
    {
        "prompt": "make two parasailers instead of one.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/da07aab9817842daaac2cb2535ce1b13_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/da07aab9817842daaac2cb2535ce1b13_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/a84d5feb22c2435485158f75b5002e1f_out.jpg",
        "model1": "PNP",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.051150570287368136,
        "model2_overall_score": 0.9475237257283453
    },
    {
        "prompt": "Can we have mountains on the background?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/98145ad56f8d4b7786658c82d118d354_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/51b12fba49bd49618ec45f33b1bb1ad0_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/98145ad56f8d4b7786658c82d118d354_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.36230311854744546,
        "model2_overall_score": 0.6607322778355849
    },
    {
        "prompt": "Make all the grass green.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/83da64356d3645b9ac4ea5262294e02f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/83da64356d3645b9ac4ea5262294e02f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/53fa5266603b41318059999b6f13ad10_out.jpg",
        "model1": "CosXLEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.057894888888965546,
        "model2_overall_score": 0.38620346724830057
    },
    {
        "prompt": "Can the man hold bananas too?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/fe444abc22a24f3db6a591dcc3f1271c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/386499ebbfbe4a3a88cb8ef6077b700b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/fe444abc22a24f3db6a591dcc3f1271c_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.691646224605103,
        "model2_overall_score": 0.5008194821520779
    },
    {
        "prompt": "let the dog yawn",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/6d41408f2ad4463aa3665aa3b1ec3567_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/124fb409642e48ab8771ddbab839b77a_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6d41408f2ad4463aa3665aa3b1ec3567_out.jpg",
        "model1": "MagicBrush",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.691438470390336,
        "model2_overall_score": 0.4987101860644114
    },
    {
        "prompt": "Make the apples green",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/775321af2f89453a8819611903c5fa9a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/775321af2f89453a8819611903c5fa9a_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/46db8da4f91242f3b132b5bea6b07c43_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5051750239255114,
        "model2_overall_score": 0.9510493230205963
    },
    {
        "prompt": "Add a cup of coffee.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/cd50498110b3432082d3f261e8e56106_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/5a527f66b24f47dfab56c072a5e04bd9_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/cd50498110b3432082d3f261e8e56106_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.1969864280001693,
        "model2_overall_score": 0.37975772128086893
    },
    {
        "prompt": "Change the woman's hair to blonde",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ad0c83dbb375435b9025ecfe83700f25_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ad0c83dbb375435b9025ecfe83700f25_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/09f8913ae5064a1394d7aba57c25b47d_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6094230196971773,
        "model2_overall_score": 0.01867644561704651
    },
    {
        "prompt": "Make it a slice of pizza instead of the sandwich.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/baa2d10e135c4a3b9f971b268524dd91_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/baa2d10e135c4a3b9f971b268524dd91_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/c0f1a753da914ea4b05f5055f13f31ba_out.jpg",
        "model1": "SDEdit",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.36312909460843823,
        "model2_overall_score": 0.6561792895570744
    },
    {
        "prompt": "remove the tent and add a bonfire",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/8c2c349a69fb4d838fafec7693bb0be4_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/344f4bb4a95e4b3083c1326c9b2bdc0f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/8c2c349a69fb4d838fafec7693bb0be4_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.2774744167334382,
        "model2_overall_score": 0.7505380035864827
    },
    {
        "prompt": "Make the man's top blue.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/4c97942326ad4896b44f4bb533486fd2_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/1f70f7ed1fd341cebd80a78032fedc04_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/4c97942326ad4896b44f4bb533486fd2_out.jpg",
        "model1": "MagicBrush",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.27573347276016913,
        "model2_overall_score": 0.9846157915054731
    },
    {
        "prompt": "add a pedestrian",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/82c2b85cc5624d31a23d1b1264a9d717_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/a12f895c081c455ab5fb4a86671ab1a7_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/82c2b85cc5624d31a23d1b1264a9d717_out.jpg",
        "model1": "MagicBrush",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.4197543978561007,
        "model2_overall_score": 0.7568273321078446
    },
    {
        "prompt": "let the cat wear a bow tie instead of a tie",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/0ae34858639c4e9787ec0e7f5c00923a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/0ae34858639c4e9787ec0e7f5c00923a_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/c795be6d23f846cfb0969b8b154e4f1d_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.11221433572006412,
        "model2_overall_score": 0.0591137656093057
    },
    {
        "prompt": "change the double deck bus to a truck",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/972f87eb93b946bbb0e2e2e689e52fc5_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/972f87eb93b946bbb0e2e2e689e52fc5_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/30492177aaa748d4be661e6b33a9638e_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3574107666391175,
        "model2_overall_score": 0.9847590063953175
    },
    {
        "prompt": "let a woman in a bridal gown stand near the cake",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/0e5488f776634951a8b3a396514b0026_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/1a1d5839a03f455d9190602d2573c68b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/0e5488f776634951a8b3a396514b0026_out.jpg",
        "model1": "SDEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.4054021736183412,
        "model2_overall_score": 0.28721109014804125
    },
    {
        "prompt": "remove the tennis ball",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/0468811cb3d246d58def24570863089d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/0468811cb3d246d58def24570863089d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/af156c501eb74d928bda73efdfe7acc7_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.20573008319051,
        "model2_overall_score": 0.34970601833852144
    },
    {
        "prompt": "let the plate contain ice cream",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/67a597986aca48a898b3b0cbff295cde_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d4991be4950148aaaa13bf4ae346bd87_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/67a597986aca48a898b3b0cbff295cde_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.13084194686387984,
        "model2_overall_score": 0.3905029578709577
    },
    {
        "prompt": "What if the man had a hat?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ae4b00f34028468e804e1a61392be78b_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/8f41ac88058940a8b2faa1fa46ad1300_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ae4b00f34028468e804e1a61392be78b_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8481005416555848,
        "model2_overall_score": 0.21563523792464
    },
    {
        "prompt": "change the teddy bear into a stuffed action figure toy",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/110011d8208841e2bf5f0f4b82c024de_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/110011d8208841e2bf5f0f4b82c024de_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/045be094ef724aa3b5581ce7773dfac7_out.jpg",
        "model1": "MagicBrush",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.32770489909722966,
        "model2_overall_score": 0.6333672215376539
    },
    {
        "prompt": "make the ramp cement",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/52d69633bb3e4e7c9322d280360aeb18_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/e9adfe050a7d43b1ad1b4fd3333f2c6b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/52d69633bb3e4e7c9322d280360aeb18_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.2397127490822164,
        "model2_overall_score": 0.002949719487981728
    },
    {
        "prompt": "remove the cloth from the chairs",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/6070dec4fd534b63813a49348732b3e5_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/6070dec4fd534b63813a49348732b3e5_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/3197074ba98141348df57b110ebf50fd_out.jpg",
        "model1": "InfEdit",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9095712989532665,
        "model2_overall_score": 0.2974769858331989
    },
    {
        "prompt": "remove the tent and add a bonfire",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/5a34cd9d41e6480994e7d65840f4944f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/5d35c1b7a87e4bb0a0969a51627cf63d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/5a34cd9d41e6480994e7d65840f4944f_out.jpg",
        "model1": "MagicBrush",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.4814912622526515,
        "model2_overall_score": 0.7097585117746663
    },
    {
        "prompt": "place a cat in the counter",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f826718707024094b21fe62c455ee915_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/2f7faf527db3417485f502ed7f1d6383_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f826718707024094b21fe62c455ee915_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.27179091036022496,
        "model2_overall_score": 0.7093221193183544
    },
    {
        "prompt": "add a lizard on the counter",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/3a4c66b3540e432aa0f6ad99f0ac2975_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ec37a065ebe34398b830d0b035033eaa_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/3a4c66b3540e432aa0f6ad99f0ac2975_out.jpg",
        "model1": "CosXLEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.42617202926660736,
        "model2_overall_score": 0.7288647603596495
    },
    {
        "prompt": "let the woman hold a cup",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/fe6b17e3a0d14bf3822c278f301953a8_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/fe6b17e3a0d14bf3822c278f301953a8_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/a62d0176c43546e68167e901db9938a1_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.14920890104168272,
        "model2_overall_score": 0.4026706066160626
    },
    {
        "prompt": "replace the chocolate with berries",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/55884724d8e24026bb07c9c921466a1f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/55884724d8e24026bb07c9c921466a1f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/d5e652c618974ae9be1861669a10f6cc_out.jpg",
        "model1": "CosXLEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5051053349602822,
        "model2_overall_score": 0.628206629920701
    },
    {
        "prompt": "change the color of the soccer ball.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/510b48222d2e4f77ab942629ee4e9886_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/510b48222d2e4f77ab942629ee4e9886_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/cb94c839c9bd4cf284347f7c2cc41e61_out.jpg",
        "model1": "CosXLEdit",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3546299058957538,
        "model2_overall_score": 0.21112867811523472
    },
    {
        "prompt": "replace the chocolate with berries",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/5cfd544b2cf34ccd8532e38a3a8b9653_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/4829068c988643d8994699ad8879aed7_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/5cfd544b2cf34ccd8532e38a3a8b9653_out.jpg",
        "model1": "PNP",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.23793567436655805,
        "model2_overall_score": 0.3061971582921642
    },
    {
        "prompt": "add flying eagles over the statue",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/377c9fec09bf4a09b7b06d8eb4f08d4e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/3995c35ffb264c00a769e40f3269c308_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/377c9fec09bf4a09b7b06d8eb4f08d4e_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6010220002762429,
        "model2_overall_score": 0.2745230712623228
    },
    {
        "prompt": "Have one of the children be blowing bubbles",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/540fad38a6e24908b263b0d15368bd85_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/540fad38a6e24908b263b0d15368bd85_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ce720d5cdeaa4c728266325cc93deaf3_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3394227285296302,
        "model2_overall_score": 0.2273016777794259
    },
    {
        "prompt": "Put a wooden floor on the kitchen.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/a894b7b0afc7493a8fa10eeebc645f3e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/a894b7b0afc7493a8fa10eeebc645f3e_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/337758d7bbc4450fb829645f76e62e76_out.jpg",
        "model1": "PNP",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8744737726603292,
        "model2_overall_score": 0.8489920456004078
    },
    {
        "prompt": "Let the bluebery cake be topped with chocolate syrup.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/3cd0256f25b3420484a0aa849fbff94b_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/3cd0256f25b3420484a0aa849fbff94b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/d4bab1a2f33d46f596b9f9140a8e356b_out.jpg",
        "model1": "InfEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3907532415076935,
        "model2_overall_score": 0.6159242657374213
    },
    {
        "prompt": "Make the zebra a regular horse.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/c6bd4550ef1c4d318a1f63e85697dcd7_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/025c41da57914c7eac6ab1a5d3b2649e_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/c6bd4550ef1c4d318a1f63e85697dcd7_out.jpg",
        "model1": "InfEdit",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8643027479862291,
        "model2_overall_score": 0.9638948820042295
    },
    {
        "prompt": "let there be potted plant",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/c8b608104a2b4625bccfe789b38497bb_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/cfd1ee368b044af2a90f22516d705038_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/c8b608104a2b4625bccfe789b38497bb_out.jpg",
        "model1": "CosXLEdit",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.41345218004660933,
        "model2_overall_score": 0.7695712479334692
    },
    {
        "prompt": "add a woman inside the car",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/31d66956acfd4f71b60c3a0e81c0e798_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/19148c17e6de4605bf327be84e674a57_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/31d66956acfd4f71b60c3a0e81c0e798_out.jpg",
        "model1": "MagicBrush",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.40100720593572325,
        "model2_overall_score": 0.4556257161467576
    },
    {
        "prompt": "add a dog barking near shore",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/0475522471f344469421ef1e665dfede_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/140f4ab878de493695273f99c93fd175_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/0475522471f344469421ef1e665dfede_out.jpg",
        "model1": "MagicBrush",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8305368548634241,
        "model2_overall_score": 0.632066410047036
    },
    {
        "prompt": "Add a cup of coffee.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/6cd341e3ff9148eea61fd8407582ecf3_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/e17afcf335e44b8089bb6f49a056550e_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6cd341e3ff9148eea61fd8407582ecf3_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8397716375024269,
        "model2_overall_score": 0.040126497225067226
    },
    {
        "prompt": "put a table on the kitchen.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/d921f7411cbc4b728302318ee9c52433_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d921f7411cbc4b728302318ee9c52433_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ed13d4d2b9fe4f85a39d1fc59ef0dd07_out.jpg",
        "model1": "MagicBrush",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.2554427507716164,
        "model2_overall_score": 0.597836201439759
    },
    {
        "prompt": "change the double deck bus to a truck",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/d439c599dcbc4147b3dc2616fd0657f1_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/a5775f8cbaae492d879c38d3db09802c_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/d439c599dcbc4147b3dc2616fd0657f1_out.jpg",
        "model1": "CosXLEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.44357121337196015,
        "model2_overall_score": 0.308911733033324
    },
    {
        "prompt": "Let the van turn black.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ba88dfd07e614dedaed39a34f391e77f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ba88dfd07e614dedaed39a34f391e77f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/550c3b2051864eaba0181af6c81b5c96_out.jpg",
        "model1": "InfEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7931252490772653,
        "model2_overall_score": 0.24202986089889278
    },
    {
        "prompt": "let the kid sleep",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/097d7d5c3adb45999cc438970a179d6a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/097d7d5c3adb45999cc438970a179d6a_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/e1882f99797741c29164d565fac61b0f_out.jpg",
        "model1": "InfEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.24141763440910924,
        "model2_overall_score": 0.9497619224689179
    },
    {
        "prompt": "put strawberry on the plate",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ac52926b0a9b414390a9e2535f217a2c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ac52926b0a9b414390a9e2535f217a2c_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/88867a9989444632b9cde038bbd75708_out.jpg",
        "model1": "PNP",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.32725832901408836,
        "model2_overall_score": 0.9853699485571717
    },
    {
        "prompt": "remove the table and add an aquarium",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/4dea3e29f8d74ee5855ebf6c44b88514_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/4dea3e29f8d74ee5855ebf6c44b88514_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/2fe7e3773c024625ae3fe48365f75d1b_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.42834023567003376,
        "model2_overall_score": 0.06641489892376673
    },
    {
        "prompt": "Can we have mountains on the background?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/a70d42722d864aa6ac1e74374fe03e5b_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/e998fa5f95744243a33a95be7ac31c07_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/a70d42722d864aa6ac1e74374fe03e5b_out.jpg",
        "model1": "PNP",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.20862196455860793,
        "model2_overall_score": 0.8241187943670222
    },
    {
        "prompt": "change the frisbee into a ball",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/fcff69e51191494081a97e233e80b322_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/fcff69e51191494081a97e233e80b322_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/14250afd91da422ca3be04c22d9059f5_out.jpg",
        "model1": "InfEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6246729536156087,
        "model2_overall_score": 0.010275179209664476
    },
    {
        "prompt": "put the zebras next to a river",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/1efeb507868c4e5cae59374da02f35d1_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/1b0d2bca4a214f85994321f338f97fb4_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/1efeb507868c4e5cae59374da02f35d1_out.jpg",
        "model1": "CosXLEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8486455852700346,
        "model2_overall_score": 0.9106822076342792
    },
    {
        "prompt": "A dog should be near the sheep.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/813442c889e54bc480b4a216cdb856a5_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/dd48e52e3e874393adc8f5a3f46e8bcf_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/813442c889e54bc480b4a216cdb856a5_out.jpg",
        "model1": "PNP",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9996352051023442,
        "model2_overall_score": 0.8554472838449717
    },
    {
        "prompt": "edit the background by removing the museum and placing a castle",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/640881c4b6cd4342a3f4fe794cc1c016_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/749fb168fac9408dae768bdf06f4da01_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/640881c4b6cd4342a3f4fe794cc1c016_out.jpg",
        "model1": "MagicBrush",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.18040775656899188,
        "model2_overall_score": 0.6529996656866084
    },
    {
        "prompt": "let the cat wear a bow tie instead of a tie",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/029532eab7ba4298bfab5efafbaa4c69_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/029532eab7ba4298bfab5efafbaa4c69_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/0bd3fe88ac7a4ee78303a7ae0c06d500_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8356589308971981,
        "model2_overall_score": 0.9941781612455345
    },
    {
        "prompt": "change the color of the soccer ball.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/01bc9ed2c59a42129629d2267e74ce7d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/01bc9ed2c59a42129629d2267e74ce7d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/8df17d13409f4655974f006086080088_out.jpg",
        "model1": "InfEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.815313508547166,
        "model2_overall_score": 0.8321256572223786
    },
    {
        "prompt": "add a bird on the road",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/50b2a4af01f046da8779d541273f62c4_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/50b2a4af01f046da8779d541273f62c4_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/88527e4d8251407bab66e664d6747b2a_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3218905943912721,
        "model2_overall_score": 0.23877589132262111
    },
    {
        "prompt": "let the dog yawn",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/0e09a45e96834043a88084ee612cedee_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/b803c8d697474c128b291fb86e4274a9_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/0e09a45e96834043a88084ee612cedee_out.jpg",
        "model1": "SDEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.1219079359389128,
        "model2_overall_score": 0.22696207938559143
    },
    {
        "prompt": "change the toothpicks into candles",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/35714144462049a0837b726dae2af5b0_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/c1ef66083b704d0eb5943a715b94edcf_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/35714144462049a0837b726dae2af5b0_out.jpg",
        "model1": "InfEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.546283626489364,
        "model2_overall_score": 0.07034169703752657
    },
    {
        "prompt": "Let the faucet be turned on.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/89b524aa8c6f421eb14cdbd42adc7e17_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/47ab45217c5f43b5b371ca9b110c4a1b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/89b524aa8c6f421eb14cdbd42adc7e17_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.09266730821614144,
        "model2_overall_score": 0.7456539519606141
    },
    {
        "prompt": "add a dog barking near shore",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/a0a2412cc0764489ba878fa4670ee793_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/6e09723bf48e438092e586a2ecceb7ae_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/a0a2412cc0764489ba878fa4670ee793_out.jpg",
        "model1": "CosXLEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.09585569064306709,
        "model2_overall_score": 0.23742658071202594
    },
    {
        "prompt": "Make it a black sheep.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/d2037283959a40398c1f5e3fb1198edc_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/0db1efbcf36842e6b55e26a4c97b5b32_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/d2037283959a40398c1f5e3fb1198edc_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.06724992908895422,
        "model2_overall_score": 0.20844316172256205
    },
    {
        "prompt": "change the double deck bus to a truck",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/37f8a5c5c3394d3385903f24bc9033a8_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/5e3f787df6c945899c6a4dc9c27c324b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/37f8a5c5c3394d3385903f24bc9033a8_out.jpg",
        "model1": "MagicBrush",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3747206160855263,
        "model2_overall_score": 0.6764505782121435
    },
    {
        "prompt": "let the cat have blue eyes",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/09d15c368bf44b018f53a5d4b0d94663_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/750d6ec555b64fc180bc8687c129b48f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/09d15c368bf44b018f53a5d4b0d94663_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.09147216853844109,
        "model2_overall_score": 0.5339755599884307
    },
    {
        "prompt": "it should be a tennis ball on the glove.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/23a8076422154f6abcb5cc7777e1f880_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/2e240203a37d47529eece902fc6cb765_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/23a8076422154f6abcb5cc7777e1f880_out.jpg",
        "model1": "SDEdit",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7226268335692487,
        "model2_overall_score": 0.5952824124726964
    },
    {
        "prompt": "let the woman cry",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/dd7133c38c994a9ebfd60e79a40e7afa_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/71e7ede2ce344600ab01ba91b7350983_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/dd7133c38c994a9ebfd60e79a40e7afa_out.jpg",
        "model1": "InfEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9264760458497725,
        "model2_overall_score": 0.5391008756037721
    },
    {
        "prompt": "give her a baseball cap",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/be42979d7c3b484583dba3c55510b97f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/4bccb05b145f42afb61cc3c05879f7eb_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/be42979d7c3b484583dba3c55510b97f_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.661256723798909,
        "model2_overall_score": 0.5970423889699381
    },
    {
        "prompt": "remove the tent and add a bonfire",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/170363eecdca4639bfa0b098bc9dac36_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/170363eecdca4639bfa0b098bc9dac36_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/249062e14b2f4076aa7b990a9bdc8f09_out.jpg",
        "model1": "MagicBrush",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.11899809340707568,
        "model2_overall_score": 0.7848565656042517
    },
    {
        "prompt": "Let the bluebery cake be topped with chocolate syrup.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/aa1367ee0f084004b22603f74ed18c2d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/13b3420dfa0643e6a828433a435f3fd5_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/aa1367ee0f084004b22603f74ed18c2d_out.jpg",
        "model1": "MagicBrush",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9027153567049253,
        "model2_overall_score": 0.10987314249187641
    },
    {
        "prompt": "add a car instead of motorcycles",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/3696cb0017be4225a25feba65069c39a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/3696cb0017be4225a25feba65069c39a_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/531365eaed3146cd89dac2aeebd580ac_out.jpg",
        "model1": "Pix2PixZero",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.37490362961778634,
        "model2_overall_score": 0.9213636083383835
    },
    {
        "prompt": "change the trees at the back into palm trees",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f17d042d03cf4d87b5d7f18bb3219682_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/e9d3409070624c88b65940501cdd8ef9_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f17d042d03cf4d87b5d7f18bb3219682_out.jpg",
        "model1": "CosXLEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7078515338538727,
        "model2_overall_score": 0.21264839217507647
    },
    {
        "prompt": "let the bed be wooden",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/7ecc44b5763f4683980eeb13b1ef7e62_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7ecc44b5763f4683980eeb13b1ef7e62_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/1c2d91757cee4b8e91b1c935b295c017_out.jpg",
        "model1": "MagicBrush",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9662575974268629,
        "model2_overall_score": 0.43947806573878057
    },
    {
        "prompt": "let the bed be wooden",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/32f3e66a2a534ce3b00e4e6bf027dac9_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/32f3e66a2a534ce3b00e4e6bf027dac9_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/cf59f7d72128421d89554096560d202d_out.jpg",
        "model1": "PNP",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6459706858404647,
        "model2_overall_score": 0.9847049568561869
    },
    {
        "prompt": "change the cat to a fox.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/2315ddd3b40d424cb503b5c472485757_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/9c086a978b73427fb41418ad70eb8ae3_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/2315ddd3b40d424cb503b5c472485757_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9311324358906534,
        "model2_overall_score": 0.3369529323137176
    },
    {
        "prompt": "There should be some cutlery on the table.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/3ed2114bb11c499cb63d7602a78200e9_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/0d44e7d41d1a4efaa2be260836fbbe18_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/3ed2114bb11c499cb63d7602a78200e9_out.jpg",
        "model1": "MagicBrush",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.4616804256665301,
        "model2_overall_score": 0.01078659172832741
    },
    {
        "prompt": "let the bed be wooden",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/3c7a6242c44a41bb88aca26cd4014c2e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/8b12fbd2e50d47cca94197c2360c5099_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/3c7a6242c44a41bb88aca26cd4014c2e_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.38097503048136716,
        "model2_overall_score": 0.810150291561105
    },
    {
        "prompt": "put the zebras next to a river",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ada1f9de11fb418e80d6243eae22ab03_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/4d5384823fea461ba4c6c0570f74aac3_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ada1f9de11fb418e80d6243eae22ab03_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.4237858507437041,
        "model2_overall_score": 0.04116018860799786
    },
    {
        "prompt": "change the toothpicks into candles",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/7572fbaf2ffb43f6a18543bfc06e6fb2_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7572fbaf2ffb43f6a18543bfc06e6fb2_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6d9e8d1ef9894b59853c335562addc2e_out.jpg",
        "model1": "CosXLEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.11545601861950994,
        "model2_overall_score": 0.9774434871641922
    },
    {
        "prompt": "remove the cloth from the chairs",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/d0c362fd87d042aeb192569c01c5d894_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f8eb55450b7546d2ab061f16bf49aa01_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/d0c362fd87d042aeb192569c01c5d894_out.jpg",
        "model1": "CosXLEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6543435438171185,
        "model2_overall_score": 0.23741619099660216
    },
    {
        "prompt": "Change the boy to a girl.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/e1a3e2b4566a429e9fc35baeb68455b9_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/70bd3bdf6a274460920e6034604f9c40_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/e1a3e2b4566a429e9fc35baeb68455b9_out.jpg",
        "model1": "Pix2PixZero",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3229401292672124,
        "model2_overall_score": 0.18152408731516256
    },
    {
        "prompt": "give the man glasses",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ae8713ff2f2149da9c5c941d45c3edc0_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/149def4569eb4a2f9af0ec4b9a255512_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ae8713ff2f2149da9c5c941d45c3edc0_out.jpg",
        "model1": "PNP",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.04462552831911348,
        "model2_overall_score": 0.494940357398827
    },
    {
        "prompt": "Make it a slice of pizza instead of the sandwich.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/022b06c07bda4dbfb5b8d3b59a9229c9_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/3b1b771360674b24843b3a30c49a5fb0_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/022b06c07bda4dbfb5b8d3b59a9229c9_out.jpg",
        "model1": "SDEdit",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3802095441544301,
        "model2_overall_score": 0.794719900953114
    },
    {
        "prompt": "Open the zebra's mouth.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/865054c2ccf44279abd5f82e1b15fe62_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ddb391d3c46d46b4b47756079a0c08c8_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/865054c2ccf44279abd5f82e1b15fe62_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9746515134869589,
        "model2_overall_score": 0.16709663411822961
    },
    {
        "prompt": "Change the woman's hair to blonde",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/72247791dcf240ce85c8c0650348745a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/36919c50ac1040289105dc7dcea6f109_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/72247791dcf240ce85c8c0650348745a_out.jpg",
        "model1": "PNP",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3669898280233992,
        "model2_overall_score": 0.20228448868279136
    },
    {
        "prompt": "Could he be in the forest?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/445d3a9d757c46c897b98a6464dd9ef2_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/445d3a9d757c46c897b98a6464dd9ef2_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/bc0f8e7f8eb94b7e97b518199d3a434c_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.1112368440351581,
        "model2_overall_score": 0.8980686513437889
    },
    {
        "prompt": "Add a cup of coffee.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/bf6d8395eab44df6b0da39ea2ab303c8_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/bf6d8395eab44df6b0da39ea2ab303c8_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/0136a115438b47e68b42ee2e4dbff051_out.jpg",
        "model1": "MagicBrush",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5632790569390214,
        "model2_overall_score": 0.0349517137902815
    },
    {
        "prompt": "change the truck into a taxi",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/6524ba53048949318ab958fe7cb8132f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/4ad14996782147da974656a7b649b02a_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6524ba53048949318ab958fe7cb8132f_out.jpg",
        "model1": "MagicBrush",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.39938397080020804,
        "model2_overall_score": 0.23060659542264572
    },
    {
        "prompt": "Add a cruise ship to the ocean.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/03bdb22c7de04d99b12d1c91416aea31_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/27bd6e3ebc6c408f8b9bda978f442255_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/03bdb22c7de04d99b12d1c91416aea31_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5128168763575854,
        "model2_overall_score": 0.9930563346717458
    },
    {
        "prompt": "Add a dear on the grass.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/bca24916fc844fa29001d83f437dbfe0_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/bca24916fc844fa29001d83f437dbfe0_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f35aa00caa404f09a09a2867719dd7a7_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.41926309384069116,
        "model2_overall_score": 0.35310729128691376
    },
    {
        "prompt": "remove bananas and add grapes",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/684e8c8e0a2b4704af775f1c11a27e23_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f14844aa232740e4bf36808b075a9a97_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/684e8c8e0a2b4704af775f1c11a27e23_out.jpg",
        "model1": "PNP",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5551541502245505,
        "model2_overall_score": 0.7570816779562344
    },
    {
        "prompt": "Put a wooden floor on the kitchen.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/018306ac8c5047df95c000be0fb6cacf_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/e96d8f61bb9540ae9092126a5f36ec14_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/018306ac8c5047df95c000be0fb6cacf_out.jpg",
        "model1": "MagicBrush",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9485396846813327,
        "model2_overall_score": 0.24576561650407758
    },
    {
        "prompt": "The bed should be red.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/cbf4d211fdb04529978840fdf4c95d3e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/9197b811567b431fb7534aa61a8c8371_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/cbf4d211fdb04529978840fdf4c95d3e_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.19585031264040764,
        "model2_overall_score": 0.7211509294297214
    },
    {
        "prompt": "Put a pond next to the cows.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/b635265914a442898b7a3f4e5a9d7c4f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/b635265914a442898b7a3f4e5a9d7c4f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/8ff44b16d50a43ad8e45e10fdda8181c_out.jpg",
        "model1": "InfEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8817701739796907,
        "model2_overall_score": 0.2598965274915225
    },
    {
        "prompt": "give the man glasses",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/9b29810eda9d435c944cacd44fbea03f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/9b29810eda9d435c944cacd44fbea03f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/14e660e0fcf4479e9bd4c13089a0269a_out.jpg",
        "model1": "MagicBrush",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.2530499148092563,
        "model2_overall_score": 0.7321086528805975
    },
    {
        "prompt": "make the plate empty",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/9b3576ca44e447d488420b261ba58e6d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/5f37912758144d2eb55d07075f7064ec_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/9b3576ca44e447d488420b261ba58e6d_out.jpg",
        "model1": "MagicBrush",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6009789662627787,
        "model2_overall_score": 0.08253317121688541
    },
    {
        "prompt": "Remove the frosting between layers.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/e5415f3adbc14a3c8760de8ff63cdb03_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/e5415f3adbc14a3c8760de8ff63cdb03_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f661af25bab64541a1f4cdd56af53ab4_out.jpg",
        "model1": "MagicBrush",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.34074882099220505,
        "model2_overall_score": 0.3198736433892925
    },
    {
        "prompt": "add a street",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/19be6fab208a413490a5b111b357be3f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/2d8bbd9fda8d42638ed5f67885fe88c0_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/19be6fab208a413490a5b111b357be3f_out.jpg",
        "model1": "InfEdit",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7271905165710715,
        "model2_overall_score": 0.6240277071430551
    },
    {
        "prompt": "Have there be a dolphin jumping out of the water",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/6ba06b643a60429ba19faa702c350389_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/b4c03385305d485eb04bc718421713b0_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6ba06b643a60429ba19faa702c350389_out.jpg",
        "model1": "MagicBrush",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9243893388004972,
        "model2_overall_score": 0.5008751189617401
    },
    {
        "prompt": "Let the bluebery cake be topped with chocolate syrup.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/645e765fdd15448ca6270b396236c9c6_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f7e023efbc4e4d2cbd381af1efa2354d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/645e765fdd15448ca6270b396236c9c6_out.jpg",
        "model1": "MagicBrush",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.37329071687037807,
        "model2_overall_score": 0.4296181509592729
    },
    {
        "prompt": "make the woman hold a camera",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/c4e55e8ddb924fb982a85f989e6e37b4_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/8aea2b4b9f174a8b9123958922fa4c48_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/c4e55e8ddb924fb982a85f989e6e37b4_out.jpg",
        "model1": "CosXLEdit",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7002306277406282,
        "model2_overall_score": 0.7562792423201212
    },
    {
        "prompt": "let the cat lay on a wooden floor instead of a carpet",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/195e00d4e7b4446dadb0624f4271f5e7_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/195e00d4e7b4446dadb0624f4271f5e7_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f019b4ba884d46da830ec0d7f0bf6f87_out.jpg",
        "model1": "InfEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.006700738071092505,
        "model2_overall_score": 0.9725809764628631
    },
    {
        "prompt": "turn the frisbee into a soccer ball",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/cdc8861818ce418e8a58a1e974bd47cb_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/23d6a04249064454abd283d8fcf19246_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/cdc8861818ce418e8a58a1e974bd47cb_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8152518676793249,
        "model2_overall_score": 0.8322737236225778
    },
    {
        "prompt": "add a car instead of motorcycles",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/1da45fe283494fa18dfe33bcf3357556_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7f051c72ce2b4100b2ec89d784b56141_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/1da45fe283494fa18dfe33bcf3357556_out.jpg",
        "model1": "InfEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9924224500004203,
        "model2_overall_score": 0.25212463901525695
    },
    {
        "prompt": "remove bananas and add grapes",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f73ed1aa247f4124931a9e28d61ecdff_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/23c97f041e4b42fb87985dbd7818a813_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f73ed1aa247f4124931a9e28d61ecdff_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9732336385881956,
        "model2_overall_score": 0.9118923127648227
    },
    {
        "prompt": "let there be potted plant",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/806515d44b0c4e978d8eb11a9948afbf_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/83f7b0b4f9d340508b290bfc38de083d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/806515d44b0c4e978d8eb11a9948afbf_out.jpg",
        "model1": "SDEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.4324131730068551,
        "model2_overall_score": 0.9331846812316817
    },
    {
        "prompt": "put a table on the kitchen.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/87c07b795dd2454785a367167aeaa54c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/c1a22750bc6048a3b7087f2449d5cda7_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/87c07b795dd2454785a367167aeaa54c_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8279680627270701,
        "model2_overall_score": 0.6050373150950741
    },
    {
        "prompt": "Remove the frisbee.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/a8479931238b4ec4bdf5f34610c5fc51_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/0dcf9f366e4c405e82c6f2ed1873e7af_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/a8479931238b4ec4bdf5f34610c5fc51_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.21467239896099133,
        "model2_overall_score": 0.2619889794739152
    },
    {
        "prompt": "change the color of the soccer ball.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/40609ab220274ef88ebcb399c81318d5_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/18bec815f8de4a89b4a37db575821a22_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/40609ab220274ef88ebcb399c81318d5_out.jpg",
        "model1": "CosXLEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.11207129458260434,
        "model2_overall_score": 0.08447404713818885
    },
    {
        "prompt": "let the cat lay on a wooden floor instead of a carpet",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/38c2d7ea1d03416b9f0469f87f11010a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/38c2d7ea1d03416b9f0469f87f11010a_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/671a0111d94246b3b5244b0ba4263142_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8073057025052767,
        "model2_overall_score": 0.6364409403702893
    },
    {
        "prompt": "change the truck into a taxi",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/5f2d1bfb96f5480bbd68994f25729c1f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d2ebf5fafd2c41899c505df911b37315_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/5f2d1bfb96f5480bbd68994f25729c1f_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.2978137651858408,
        "model2_overall_score": 0.08380783992169627
    },
    {
        "prompt": "Put a wooden floor on the kitchen.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ffd23ab49c3346d3988a15d224c14f37_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/1e9843ea85a04b788ded99347c1c0b5e_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ffd23ab49c3346d3988a15d224c14f37_out.jpg",
        "model1": "CosXLEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3486566888739844,
        "model2_overall_score": 0.23330799506192146
    },
    {
        "prompt": "Can we have mountains on the background?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ed6bd573690f44809893711bbf5e1eb0_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/0125c927d0e541e2aff459bb36ec3539_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ed6bd573690f44809893711bbf5e1eb0_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.990707582494526,
        "model2_overall_score": 0.9998089789426481
    },
    {
        "prompt": "add another cup of water",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/fc532153646f4573bd4f17abceedcb09_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/17cbad648bd74685bfaab3783fd47e03_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/fc532153646f4573bd4f17abceedcb09_out.jpg",
        "model1": "CosXLEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.4478760150232163,
        "model2_overall_score": 0.7187938278701709
    },
    {
        "prompt": "let the dog have a newspaper in its mouth",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/483c3b1d6e2d46b882f0425f6c19d34c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d1f593f5f11c4116bf08262253cb7acb_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/483c3b1d6e2d46b882f0425f6c19d34c_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.1897947552057001,
        "model2_overall_score": 0.8683691813828113
    },
    {
        "prompt": "change the stop sign to a welcome sign",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/32ef5094bb3445e7a41d45050949e7ca_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/3d551f3048c54bc19f2d48d88db49170_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/32ef5094bb3445e7a41d45050949e7ca_out.jpg",
        "model1": "CosXLEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9113501217923161,
        "model2_overall_score": 0.5488194561769182
    },
    {
        "prompt": "place a cat in the counter",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/0551e30ab04e4bdf9f09b17bfd158c6d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/0551e30ab04e4bdf9f09b17bfd158c6d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/dc74604dacd84a149c6db0637b6a85af_out.jpg",
        "model1": "InfEdit",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3614136383388661,
        "model2_overall_score": 0.8948843471221708
    },
    {
        "prompt": "change the Washington Monument with the Statue of Liberty",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/51d98d1e531144de82da97cc4739e015_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/51d98d1e531144de82da97cc4739e015_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/075acf69a805434a83522534e61a786a_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.2558435030561592,
        "model2_overall_score": 0.3773735680038304
    },
    {
        "prompt": "let a herd of sheep block the taxi",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/a61780ff66bd4b4cbf8a2b3734dbedfb_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/8a6e821847094092b6670cae134eb459_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/a61780ff66bd4b4cbf8a2b3734dbedfb_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7708035134267545,
        "model2_overall_score": 0.29107781640269037
    },
    {
        "prompt": "Make the donut a cupcake.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/9064b1e3c40b41a9832c6b1c13a9006a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7c6575082c7e4147b1a10c7ec3742fce_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/9064b1e3c40b41a9832c6b1c13a9006a_out.jpg",
        "model1": "SDEdit",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5884941186144848,
        "model2_overall_score": 0.1992455533109807
    },
    {
        "prompt": "remove the table and add an aquarium",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/95c6d0c068cd41f19bc75146082010e6_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7c0fd6415f0a4843ba32877418564a37_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/95c6d0c068cd41f19bc75146082010e6_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.1027286396194762,
        "model2_overall_score": 0.6349281109983514
    },
    {
        "prompt": "add a lizard on the counter",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/b51afd65d5ed47469e5fd5481c99084a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/a7d9db01b1384549a7e4492a0bb4d855_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/b51afd65d5ed47469e5fd5481c99084a_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6556896307787389,
        "model2_overall_score": 0.9331197049580591
    },
    {
        "prompt": "change the stop sign to a welcome sign",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/43af35cfc0db4b72bac7c3e4e0aaa7fe_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/43af35cfc0db4b72bac7c3e4e0aaa7fe_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/1a8f85cad8f84fba8a6d3e26b85395da_out.jpg",
        "model1": "MagicBrush",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.12255914318775507,
        "model2_overall_score": 0.3507394514037617
    },
    {
        "prompt": "add a lizard on the counter",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/01789926560241a69b8787231c0afb1c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/40ebc544f7704fa887860285dd9693b8_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/01789926560241a69b8787231c0afb1c_out.jpg",
        "model1": "CosXLEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5498434586613405,
        "model2_overall_score": 0.8162235907814628
    },
    {
        "prompt": "give the man glasses",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/02bd1bdb789345e69120c8a35b153eed_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/393beac31526440e8939579b6ea2970f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/02bd1bdb789345e69120c8a35b153eed_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.47186569451630345,
        "model2_overall_score": 0.014597590744031441
    },
    {
        "prompt": "Make the apples green",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/8b09e66573b64a8db97e8dc309234b52_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/8b09e66573b64a8db97e8dc309234b52_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/2d65594d3ed449569184fec530644a2b_out.jpg",
        "model1": "PNP",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.39064588239467923,
        "model2_overall_score": 0.07347396931442451
    },
    {
        "prompt": "turn the remote into a pizza",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/d4be3d107d304dcc8d19bd0302de3826_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d4be3d107d304dcc8d19bd0302de3826_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/8632216d47f14d67856cf8b0d1da8be9_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.06754242109022979,
        "model2_overall_score": 0.012207882826566818
    },
    {
        "prompt": "place a cat in the counter",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/115fbdfa96df4a7fa45760cec68cf0f2_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/115fbdfa96df4a7fa45760cec68cf0f2_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/033bfc8e0b7343f8953a92f769f4bf82_out.jpg",
        "model1": "InfEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.16175713181021178,
        "model2_overall_score": 0.39274479724700917
    },
    {
        "prompt": "put a party hat on the dog",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/3922f681803943d2950bcd202dcd254f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/06820eb9725448fc92f16a423158c14d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/3922f681803943d2950bcd202dcd254f_out.jpg",
        "model1": "MagicBrush",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3267255845963867,
        "model2_overall_score": 0.815723536476086
    },
    {
        "prompt": "give the man glasses",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/1136a324c7094e75a7034bc2ba7d7c7f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/c2ebc91138a641b0a4a925d34cab7713_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/1136a324c7094e75a7034bc2ba7d7c7f_out.jpg",
        "model1": "CosXLEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.014310017031933353,
        "model2_overall_score": 0.2415984514253341
    },
    {
        "prompt": "Make the dog's eyes closed.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/1f88443321dc4a7bb6ba9d5f8890e8ab_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/1f88443321dc4a7bb6ba9d5f8890e8ab_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/57627f7f8b234c15924d2aa00c545c1f_out.jpg",
        "model1": "InfEdit",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5301170494396807,
        "model2_overall_score": 0.5858575803703867
    },
    {
        "prompt": "Put a show about cats on the TV.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/a9241ddfa97243ad83b1cc59fb02f6e3_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/a9241ddfa97243ad83b1cc59fb02f6e3_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/87a7638ec9334cf5aca2ab23e17048f3_out.jpg",
        "model1": "InfEdit",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.49025391275196206,
        "model2_overall_score": 0.5971195974010013
    },
    {
        "prompt": "add a rocket in the sky",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f2a3f41af47e400a82a807ead4498383_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ecd0115eb6614767872b9def316ab001_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f2a3f41af47e400a82a807ead4498383_out.jpg",
        "model1": "MagicBrush",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8972220386003887,
        "model2_overall_score": 0.9426999381851381
    },
    {
        "prompt": "give the man glasses",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/fd0ca1e44a364d5dbf5dfc41c692815c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/15f2e4c5984541599575f4f1a1b8c727_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/fd0ca1e44a364d5dbf5dfc41c692815c_out.jpg",
        "model1": "CosXLEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9210413218316323,
        "model2_overall_score": 0.03863625025392914
    },
    {
        "prompt": "add a polar bear",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/d078d1f6345a40c281d73b224dd4b1b0_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d078d1f6345a40c281d73b224dd4b1b0_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/dd6acf3483c749418e9b07b9b357d599_out.jpg",
        "model1": "InfEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.2451635729785816,
        "model2_overall_score": 0.25637633023138495
    },
    {
        "prompt": "What if the child was holding a bottle of peper?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/4e7781f602354944b68153620578faf1_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/4e7781f602354944b68153620578faf1_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/bb8b8bbf4a2e4dbd87359394efc28561_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5888677293801922,
        "model2_overall_score": 0.11465681499507174
    },
    {
        "prompt": "Make the doll wear a hat.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/aff4eee15af1491ab1e47def3121c1a1_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/68543952028849a2851085ff1650935a_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/aff4eee15af1491ab1e47def3121c1a1_out.jpg",
        "model1": "MagicBrush",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.27494828683619044,
        "model2_overall_score": 0.8270648405317826
    },
    {
        "prompt": "Make the dog's eyes closed.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/a63beb8ae2f24901b0fc4b841ba745c1_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/a63beb8ae2f24901b0fc4b841ba745c1_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/a9a4c51bdad64dbf9219fb22f3c3e70d_out.jpg",
        "model1": "InfEdit",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5918531373610576,
        "model2_overall_score": 0.8635089726012333
    },
    {
        "prompt": "remove middle fruit and put a cat in place",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/b7a0d61d40e34a68876fefdb98214970_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/b72142dd7975450e8e8d6f9d51688050_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/b7a0d61d40e34a68876fefdb98214970_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.11808967916878399,
        "model2_overall_score": 0.22982366933108844
    },
    {
        "prompt": "change the clock tower to a bell tower",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/55279fe998ad4a8e96166b3d6e8fc1d7_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/55279fe998ad4a8e96166b3d6e8fc1d7_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/e733675c06d64365981c6ce5148bdadd_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.02948874395595502,
        "model2_overall_score": 0.5741720676656418
    },
    {
        "prompt": "Add a cup of coffee.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/9b258dd6f44947cba36f248dbd67ad26_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/9b258dd6f44947cba36f248dbd67ad26_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/0d84b6a1bf7f43be9d49ef3e9425506b_out.jpg",
        "model1": "MagicBrush",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.14698480981265327,
        "model2_overall_score": 0.6263019663737094
    },
    {
        "prompt": "make the plate empty",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ed239d7e453c4b5db6db0606c3a2d760_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ed239d7e453c4b5db6db0606c3a2d760_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/1c32f0e74bb243c4875ef9c1346a71a1_out.jpg",
        "model1": "MagicBrush",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.4738396533370304,
        "model2_overall_score": 0.5969901078282659
    },
    {
        "prompt": "Put a pond next to the elephant",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/7a7d5ba6d18c4978995e65db96772819_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/a8a36599f89a46e684cdf0f645dcaf5b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/7a7d5ba6d18c4978995e65db96772819_out.jpg",
        "model1": "InfEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.3942626201951107,
        "model2_overall_score": 0.2528632273348952
    },
    {
        "prompt": "change the green ball to blue",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/af190fb4192a4cfd9c60c3bb0638ce74_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/a8068fbf58c94f65a5d18550f7773a29_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/af190fb4192a4cfd9c60c3bb0638ce74_out.jpg",
        "model1": "PNP",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.15180382954259453,
        "model2_overall_score": 0.7079383961539256
    },
    {
        "prompt": "Remove the frisbee.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/caa2aba0557447139e59f0eecfde821e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/caa2aba0557447139e59f0eecfde821e_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/61d1cfd7fbe1406b8bf7f433b40e7182_out.jpg",
        "model1": "MagicBrush",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.31434186213451487,
        "model2_overall_score": 0.9566785600437065
    },
    {
        "prompt": "change the teddy bear into a stuffed action figure toy",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/280ca1ec206e410381531d72e889bf4c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/88b9227597f54168a759903acc2e5e0b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/280ca1ec206e410381531d72e889bf4c_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8031058741550715,
        "model2_overall_score": 0.9915522036940013
    },
    {
        "prompt": "edit the background by removing the museum and placing a castle",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/7e3b4950e25041e89cb2f93d90e0e9c8_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f2a61f67c7244dfc9a0c6b2a70667b27_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/7e3b4950e25041e89cb2f93d90e0e9c8_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9275640512824525,
        "model2_overall_score": 0.9417211674813449
    },
    {
        "prompt": "Have there be a dolphin jumping out of the water",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/231612540cd64957aaf8aa55347d98a2_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/231612540cd64957aaf8aa55347d98a2_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/61472ce404734f25bc2907600d00a5fa_out.jpg",
        "model1": "MagicBrush",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8876979553309369,
        "model2_overall_score": 0.9509608507999371
    },
    {
        "prompt": "make it a pepperoni pizza",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/805efa9291324121ac2fe24172ff2872_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/0143394aa1ce4754ab04b9be6ff024fc_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/805efa9291324121ac2fe24172ff2872_out.jpg",
        "model1": "PNP",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5098733118195783,
        "model2_overall_score": 0.49972825515370833
    },
    {
        "prompt": "Make the zebra a regular horse.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/cb3c08431a1b4af3874ceccefbe8b0dc_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/cb3c08431a1b4af3874ceccefbe8b0dc_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/041be215dbc74d50bf25244e571479c7_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.20943374956278715,
        "model2_overall_score": 0.25124804745921014
    },
    {
        "prompt": "Can we have a blue airplane?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ca99672f7c484dbaa6ad52de461ea22d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/12685328486c463aa7378d0ea1fdf33c_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ca99672f7c484dbaa6ad52de461ea22d_out.jpg",
        "model1": "SDEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7946397779559741,
        "model2_overall_score": 0.6937753701087491
    },
    {
        "prompt": "let the cat lay on a wooden floor instead of a carpet",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/6cfdb943055b4f8687fc1b78fadc9324_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/975f6eb876294897be7d6a997b500bb4_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6cfdb943055b4f8687fc1b78fadc9324_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.32348976301814136,
        "model2_overall_score": 0.27161191530067086
    },
    {
        "prompt": "Put a rat on the counter.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/08b0c76de2e84b148dcf2684dd262e9c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/08b0c76de2e84b148dcf2684dd262e9c_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/de1ed7ca9e7642f983e63c5ad70384a5_out.jpg",
        "model1": "CosXLEdit",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.37406643650461546,
        "model2_overall_score": 0.6422256029616278
    },
    {
        "prompt": "Could he be in the forest?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/d0032c4e48814dfc9c26b53cf22618af_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/d0032c4e48814dfc9c26b53cf22618af_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ad303fdd8bfc46d6b33c17ed833848bb_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9839996251354779,
        "model2_overall_score": 0.8506043281586642
    },
    {
        "prompt": "put another freezer on the truck",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/f19ca08df5ee4d6c9e3332ec382ce6ad_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/bdb6b177a1f04ae2aca478bf8bd06143_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f19ca08df5ee4d6c9e3332ec382ce6ad_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8939097481643323,
        "model2_overall_score": 0.5927891197642445
    },
    {
        "prompt": "add a rocket in the sky",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/8cc79ba4c6cb4e9bb613a1b70a9999d6_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/a37317d9eba74bdeb3c38d8fb21b0f4c_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/8cc79ba4c6cb4e9bb613a1b70a9999d6_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.4121748237583035,
        "model2_overall_score": 0.959210348132027
    },
    {
        "prompt": "make it a pepperoni pizza",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/ea90fc33023e4fffb896b555c557bcc6_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ea90fc33023e4fffb896b555c557bcc6_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/e224e67f905a49abb995dcd24f6a93f7_out.jpg",
        "model1": "PNP",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9409485251066098,
        "model2_overall_score": 0.6731916635364094
    },
    {
        "prompt": "add a polar bear",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/856048e940c44bfda7927e06ebdd0e15_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f023e8386ad64a3cb737362b58bc6128_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/856048e940c44bfda7927e06ebdd0e15_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8715335791243218,
        "model2_overall_score": 0.5621725712240211
    },
    {
        "prompt": "Let's add a monitor on the wall.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/0aefa4b1aa7248d7b1656ac808c11d1a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/bc38508ed07b4a309106489517dc07cc_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/0aefa4b1aa7248d7b1656ac808c11d1a_out.jpg",
        "model1": "CosXLEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.49369577924950125,
        "model2_overall_score": 0.8969741907733858
    },
    {
        "prompt": "Change the trees to palm trees",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/77c85591922743109b8bef028d0eaa86_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/77c85591922743109b8bef028d0eaa86_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/3dfbea9f66734eeabb339f61ede2bfa4_out.jpg",
        "model1": "PNP",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.17728911814899562,
        "model2_overall_score": 0.45227869084189687
    },
    {
        "prompt": "have the person jump over a tennis ball.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/9be95fce7d9d47c294bc454b2e0d7948_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/9be95fce7d9d47c294bc454b2e0d7948_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/f2723ff698eb4fe289937c4ccf4c4a6f_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7284746703941791,
        "model2_overall_score": 0.7777024517627422
    },
    {
        "prompt": "change the cat to a fox.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/332bba631a5441eaaaf28dda3163a28b_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/332bba631a5441eaaaf28dda3163a28b_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/bb20f36c407642fe98b3168540e2f151_out.jpg",
        "model1": "InfEdit",
        "model2": "MagicBrush",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.12860136799356425,
        "model2_overall_score": 0.24922280933379115
    },
    {
        "prompt": "give the girl a baseball bat instead of a racket",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/2ec3a06dec444684a83a32eafe68b69c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/2ec3a06dec444684a83a32eafe68b69c_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/dc25bbef3a5f4c95ac348a8f98d23cfd_out.jpg",
        "model1": "InfEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6554301599315885,
        "model2_overall_score": 0.4037842300164095
    },
    {
        "prompt": "Add a cruise ship to the ocean.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/11b479a76b5f4242ad6c5cca0d09f70f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/11b479a76b5f4242ad6c5cca0d09f70f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/2d45902ae4a643e6a2b8331ca34acd36_out.jpg",
        "model1": "PNP",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.31504814603932674,
        "model2_overall_score": 0.5618127298285789
    },
    {
        "prompt": "change the ski gear into a scuba gear",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/a0859e3573de4eefbdb3691c8a02f173_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/b70e6cb2b9d545f3848a5afdfc5e2220_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/a0859e3573de4eefbdb3691c8a02f173_out.jpg",
        "model1": "CosXLEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.06396112596592152,
        "model2_overall_score": 0.843512821270386
    },
    {
        "prompt": "Get rid of the jar of cookies",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/aeb8aada2e1b4071b3348401a6886a2d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/114191d89e804cfc8083d8529b65d974_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/aeb8aada2e1b4071b3348401a6886a2d_out.jpg",
        "model1": "MagicBrush",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.47874767178529287,
        "model2_overall_score": 0.188531144421211
    },
    {
        "prompt": "add a dog barking near shore",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/6b5eefa4120540b2b942b806df478b27_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/6b5eefa4120540b2b942b806df478b27_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/e8199c26bebb4266bcb0220b764a2bed_out.jpg",
        "model1": "MagicBrush",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.012967215435669588,
        "model2_overall_score": 0.0848369449992813
    },
    {
        "prompt": "change the trees at the back into palm trees",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/65f0816422a8497da150ca9b32db04bc_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/65f0816422a8497da150ca9b32db04bc_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6e5674c805da41d7b7ebae39ac34c6ed_out.jpg",
        "model1": "InfEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5552709018008615,
        "model2_overall_score": 0.31269330939506323
    },
    {
        "prompt": "make it a pepperoni pizza",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/65209662019d467694669487fa7a128c_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/65209662019d467694669487fa7a128c_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/11197f35a75043ed9e3afa7242a9b9d9_out.jpg",
        "model1": "CosXLEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9922967336953297,
        "model2_overall_score": 0.5215451989862926
    },
    {
        "prompt": "let the cat lay on a wooden floor instead of a carpet",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/1e335d131d4b44fe8df6bd9d89840564_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/1e335d131d4b44fe8df6bd9d89840564_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/5c3990930bf44976a7918ed2b719b52e_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.16582989685694183,
        "model2_overall_score": 0.9078838439749369
    },
    {
        "prompt": "put a robot tiger next to the bear",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/7faf2b708d8c4e7fa3e9ae1587a52db6_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7faf2b708d8c4e7fa3e9ae1587a52db6_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/e36db1adba4541b28cb239ceca4428d8_out.jpg",
        "model1": "SDEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.40835649181672884,
        "model2_overall_score": 0.8313727526978688
    },
    {
        "prompt": "replace the chocolate with berries",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/9bf877a9f5444b9c8beb69a02b404c6e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/9bf877a9f5444b9c8beb69a02b404c6e_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/e0e4f08113524608af46fe54f1d67a5f_out.jpg",
        "model1": "PNP",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.1762111396017163,
        "model2_overall_score": 0.6755735162230426
    },
    {
        "prompt": "let the cat wear a bow tie instead of a tie",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/6477eab85b184a0f894b58c9ca690972_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/65faaab3e813434a9682b9486faa6c00_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6477eab85b184a0f894b58c9ca690972_out.jpg",
        "model1": "MagicBrush",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.0191915758173119,
        "model2_overall_score": 0.2696122924799912
    },
    {
        "prompt": "edit the background by removing the museum and placing a castle",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/59a15fbb0c8943f59a82c25961ce2306_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/59a15fbb0c8943f59a82c25961ce2306_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/ebf3a289493f4eabba01a0c3720632fe_out.jpg",
        "model1": "Prompt2prompt",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6504366201000664,
        "model2_overall_score": 0.6471421347274041
    },
    {
        "prompt": "Let the man press the keyboard.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/20c3ce71c934433d9435166d425449f8_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/20c3ce71c934433d9435166d425449f8_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/158f22d2a9a84774892359ed9054f8f6_out.jpg",
        "model1": "SDEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9176417211401074,
        "model2_overall_score": 0.29014970412400887
    },
    {
        "prompt": "edit some mountains in the background",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/e6f3c52107904d68baf67f06c137abe8_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/e6f3c52107904d68baf67f06c137abe8_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/277ee66d9a8e49e997a50a12306b12f3_out.jpg",
        "model1": "MagicBrush",
        "model2": "InstructPix2Pix",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5652028545657088,
        "model2_overall_score": 0.8407303796943291
    },
    {
        "prompt": "Can we have a blue airplane?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/481a273916ab4750a298132155cb6e13_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/32e12e1aa78e4619b00761b27674e1a7_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/481a273916ab4750a298132155cb6e13_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "InfEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.20765939560475177,
        "model2_overall_score": 0.9876237947364332
    },
    {
        "prompt": "Can we have mountains on the background?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/2dd98ff8df8c4836899ced7be4a43729_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/363e25229f8e478ca9bdc5c3dc2722f5_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/2dd98ff8df8c4836899ced7be4a43729_out.jpg",
        "model1": "InfEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8405654221847998,
        "model2_overall_score": 0.22345745630985547
    },
    {
        "prompt": "Put popcorn in the plate.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/6928068362694523aa57ca1551d3fd98_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/6928068362694523aa57ca1551d3fd98_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/c6d620ac8c424d05bfde8e8ba81c972f_out.jpg",
        "model1": "PNP",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.688548544726625,
        "model2_overall_score": 0.12968251142921094
    },
    {
        "prompt": "Let the monitor turn black.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/94648de53b08464d8cec4c0fb7a39996_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/94648de53b08464d8cec4c0fb7a39996_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/c3b48067ba3f46dd8b9f2e6e2117c48b_out.jpg",
        "model1": "SDEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9983956967662241,
        "model2_overall_score": 0.22072874266442133
    },
    {
        "prompt": "give the man glasses",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/7bfba3f40ba54151acdff6b62da7d11d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/ec60e2152a674e5e80b51f2580e8565a_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/7bfba3f40ba54151acdff6b62da7d11d_out.jpg",
        "model1": "CosXLEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8397238588905264,
        "model2_overall_score": 0.17618351805102594
    },
    {
        "prompt": "put a table on the kitchen.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/409418a14386491c908f0ba39d25c650_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/409418a14386491c908f0ba39d25c650_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/b9739eb5d9834ffca634a47d70ad8bea_out.jpg",
        "model1": "MagicBrush",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8306821662699343,
        "model2_overall_score": 0.3387815972656565
    },
    {
        "prompt": "let the woman kiss",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/067dc3c876a245a7b66cb300d8535a66_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/6349b188d161445dabd855373dc82c2d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/067dc3c876a245a7b66cb300d8535a66_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.16616750087693744,
        "model2_overall_score": 0.048883842648194564
    },
    {
        "prompt": "make the cat lick its nose",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/32d3b9b3b6604d8aae5953632fde357e_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/f1790a76543d40479e9850896c0a8451_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/32d3b9b3b6604d8aae5953632fde357e_out.jpg",
        "model1": "CycleDiffusion",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7712745717657999,
        "model2_overall_score": 0.7611679079569301
    },
    {
        "prompt": "add a polar bear",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/41c867f36153427b886a5ac315db2c61_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/4059d37ebfcf4b8b90e478b6fccbb61d_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/41c867f36153427b886a5ac315db2c61_out.jpg",
        "model1": "CosXLEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8440925483697895,
        "model2_overall_score": 0.8916484209055299
    },
    {
        "prompt": "Can we have a blue airplane?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/acb127ce4d284c119d61aee675595ae0_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/acb127ce4d284c119d61aee675595ae0_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/45d76b3a2aad48c2ab2e7414f3e47322_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.2751570498892175,
        "model2_overall_score": 0.1266970177541088
    },
    {
        "prompt": "There should be a tree on the front.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/3b4ae7140f4842d7b564431aa9fced18_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/3b4ae7140f4842d7b564431aa9fced18_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/945a441679954caf898919a8921e7c82_out.jpg",
        "model1": "PNP",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.21294302434974788,
        "model2_overall_score": 0.23827998782617177
    },
    {
        "prompt": "Make the cake a chocolate cake",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/17fc7c114e36405f8bd6c2455b3185df_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/17fc7c114e36405f8bd6c2455b3185df_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/39ceef4f9289476ab4960b43c6fd36ad_out.jpg",
        "model1": "CosXLEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7401184181771399,
        "model2_overall_score": 0.24870291540974543
    },
    {
        "prompt": "replace the chocolate with berries",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/cb5051d3f93044bfa37a13bd9217d201_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/cb5051d3f93044bfa37a13bd9217d201_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/1dc9363f5a39401591de77dc528b9d5e_out.jpg",
        "model1": "PNP",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9727513355524126,
        "model2_overall_score": 0.7983361024100747
    },
    {
        "prompt": "Add a cup of coffee.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/fd20d731fc28493d8cd85281a271f55f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/fd20d731fc28493d8cd85281a271f55f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/41c13c97aa2f44ecae2f24e577bd1e0d_out.jpg",
        "model1": "SDEdit",
        "model2": "CosXLEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7524196845128425,
        "model2_overall_score": 0.9840825186966651
    },
    {
        "prompt": "replace the chocolate with berries",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/8c643af22b2f4ecab6d084c07752db0f_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/8c643af22b2f4ecab6d084c07752db0f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/63640530621d421bb97d667de79face2_out.jpg",
        "model1": "CosXLEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5366258503384402,
        "model2_overall_score": 0.3152328755882262
    },
    {
        "prompt": "remove the clouds",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/0b31b8c2802b4a54b021950e32080ac8_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/0b31b8c2802b4a54b021950e32080ac8_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/afcf82f257f5417ab1cbe1f1b62d7847_out.jpg",
        "model1": "PNP",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.37991172755764835,
        "model2_overall_score": 0.8276614400894293
    },
    {
        "prompt": "turn the remote into a pizza",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/7370f3bc9b444a5faf830ff039beeb6a_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/7370f3bc9b444a5faf830ff039beeb6a_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/df8d3689bc51421d9b7d55c9bdab1f6b_out.jpg",
        "model1": "InfEdit",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.6438954110895326,
        "model2_overall_score": 0.14523485788467783
    },
    {
        "prompt": "Can we have mountains on the background?",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/18aee42cdbcd40778b474ab15803f680_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/18aee42cdbcd40778b474ab15803f680_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6f5dcfad76744358b021fa2715503ab4_out.jpg",
        "model1": "InfEdit",
        "model2": "Pix2PixZero",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9956297990385912,
        "model2_overall_score": 0.1742748940136446
    },
    {
        "prompt": "let the woman kiss",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/07712330685b48c9a1ab1e95ee99c179_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/473a4239e99d4c5bbdd06486b483aa80_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/07712330685b48c9a1ab1e95ee99c179_out.jpg",
        "model1": "InfEdit",
        "model2": "PNP",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.46480687918210084,
        "model2_overall_score": 0.9663135994230385
    },
    {
        "prompt": "add water and flowers in the tub",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/68e0ceda14cf4957a1779b8415ee9e28_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/68e0ceda14cf4957a1779b8415ee9e28_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/9be4d288fe89493cb65a90db3b0342aa_out.jpg",
        "model1": "InstructPix2Pix",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5167068014204952,
        "model2_overall_score": 0.9978238826163986
    },
    {
        "prompt": "remove the cloth from the chairs",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/9baba0307fb944e1a2fa4d274d6849fc_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/9baba0307fb944e1a2fa4d274d6849fc_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/2f37ecb2c8de49d38cc4e505ccaa35d9_out.jpg",
        "model1": "PNP",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.9902567028558974,
        "model2_overall_score": 0.7575678409459303
    },
    {
        "prompt": "make the cat's nose black",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/97acced2ee2d4efaadc99b159c6c5ddb_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/97acced2ee2d4efaadc99b159c6c5ddb_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/b28709bf9f1c43838b2da37a6a9ead22_out.jpg",
        "model1": "CosXLEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.5770461781976796,
        "model2_overall_score": 0.8838304728269802
    },
    {
        "prompt": "turn the frisbee into a soccer ball",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/fc7d043fcd56432fbb4dae14625bdd63_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/4be3d7cecc064e3aa739430c727d04b9_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/fc7d043fcd56432fbb4dae14625bdd63_out.jpg",
        "model1": "InfEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.854572977471115,
        "model2_overall_score": 0.30768425732078564
    },
    {
        "prompt": "Open the zebra's mouth.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/6029104d20f34b7581127bb36142b921_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/4c07d30744e84c9fb7e2fbf0043a5e31_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/6029104d20f34b7581127bb36142b921_out.jpg",
        "model1": "MagicBrush",
        "model2": "CycleDiffusion",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.8984067509065576,
        "model2_overall_score": 0.2548983420159542
    },
    {
        "prompt": "Put a show about cats on the TV.",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/8c397022e87749d1931ab24eacaee852_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/72da35bffebb4516b22df18ee523f8ee_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/8c397022e87749d1931ab24eacaee852_out.jpg",
        "model1": "InfEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.31536611881370724,
        "model2_overall_score": 0.4439185282134901
    },
    {
        "prompt": "Have the woman be wearing a blue tank top",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/4406b6e61e694b5f9a83b1df03fa115d_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/3e5d8c85531e4d6da2703a62fb2ffe72_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/4406b6e61e694b5f9a83b1df03fa115d_out.jpg",
        "model1": "InfEdit",
        "model2": "Prompt2prompt",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.7450713113890038,
        "model2_overall_score": 0.7840474518034003
    },
    {
        "prompt": "let there be a game show on TV",
        "choice_dist": null,
        "confidence": null,
        "path_src": "GenAI-Bench/image_edition/images/737693160cb6409ba7f430f5e048def7_src.jpg",
        "path1": "GenAI-Bench/image_edition/images/92b5fbf71801419e8bdb516c8530086f_out.jpg",
        "path2": "GenAI-Bench/image_edition/images/737693160cb6409ba7f430f5e048def7_out.jpg",
        "model1": "CosXLEdit",
        "model2": "SDEdit",
        "model1_dim1_score": 4,
        "model2_dim1_score": 2,
        "model1_overall_score": 0.47325526914456173,
        "model2_overall_score": 0.5910326294773727
    }
]